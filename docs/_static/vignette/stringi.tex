% kate: indent-width 2; word-wrap-column 74

% knitr::knit("stringi_v2.Rnw", tangle=TRUE)



%!TEX program = XeLaTeX
%!Rnw weave = Sweave

% sudo apt install latex-cjk-all

\documentclass[nojss]{jss}
% \documentclass[article]{jss}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{Sweave}


\usepackage{xltxtra}

\usepackage{thumbpdf,lmodern,xcolor}
% \usepackage{framed}
% \usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
% \usepackage{CJKutf8}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{tabularx}
% \usepackage[UTF8]{ctex}
\usepackage{xeCJK}
\usepackage{fontspec}
% \usepackage{polyglossia}
\setCJKmainfont{Noto Sans Mono CJK SC}
\setCJKsansfont{Noto Sans Mono CJK SC}
\setCJKmonofont{Noto Sans Mono CJK SC}
% \usepackage{lmodern}
% \setmonofont{Latin Modern Mono}
% \setmonofont{Ubuntu Mono}%[Scale=0.85]
% \usepackage{alphabeta}

\newcommand{\strq}[1]{\code{{"{}#1"{}}}}
\newcommand{\str}[1]{\code{{#1}}}
\usepackage{tipa}


%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
%\usepackage{Sweave}
%\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}





\author{Marek Gagolewski\\
Deakin University, Australia} % \emph{and} Warsaw University of Technology}
\Plainauthor{Marek Gagolewski}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\pkg{stringi}: Fast and Portable\\Character String
Processing in \proglang{R}}
\Plaintitle{stringi: Fast and Portable Character String Processing in R}
\Shorttitle{\pkg{stringi}: Fast and Portable Character String Processing
in \proglang{R}}

% \title in title style,
% all titles in the BibTeX file in title style.
% \section, \subsection, etc. in sentence style,



%% - \Abstract{} almost as usual
\Abstract{
Effective processing of character strings is required at various
stages of data analysis pipelines:
from data cleansing and preparation, through
information extraction, to report generation.
Pattern searching, string collation and sorting, normalisation,
transliteration, and formatting are ubiquitous in
text mining, natural language processing, and bioinformatics.
This paper discusses and demonstrates how and why \pkg{stringi},
a mature \proglang{R} package for fast and portable handling of string data,
should be included in each statistician's or data scientist's
repertoire.
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{\pkg{stringi}, character strings, text, \pkg{ICU}, Unicode,
regular expressions, data cleansing, natural language processing,
\proglang{R}}
\Plainkeywords{stringi, character strings, text, ICU, Unicode, regular
expressions, data cleansing, natural language processing, R}

\Address{
  \textbf{Marek Gagolewski}\\
  School of Information Technology\\
  Deakin University\\
  Geelong, VIC 3220, Australia\\
  %\emph{and}\\
  %Faculty of Mathematics and Information Science\\
  %Warsaw University of Technology\\
  %ul.~Koszykowa 75, 00-662 Warsaw, Poland\\
  E-mail: \email{m.gagolewski@deakin.edu.au}\\
  URL: \url{https://www.gagolewski.com/}
}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}


{\color{blue}
This is a draft version of a paper on \pkg{stringi},
last updated on \today.

Please cite as:
Gagolewski M (2020).
\textit{\pkg{stringi}: Fast and Portable Character String Processing in \proglang{R}}.
URL \url{https://stringi.gagolewski.com}.
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{Sec:intro}

Stringology, see \citep{stringology},
deals with the algorithms and data structures used for processing
of character strings \citep{speechlangproc,szpankowski}.
From the perspective of applied statistics and data science,
it is worth stressing that many interesting datasets
first come in an unstructured or contaminated form,
for instance when fetched from different APIs or when gathered
by means of web scraping techniques.
Diverse data cleansing and preparation operations
(\citealp{datacleaning,cleanr}; see also
this paper's Section~\ref{Sec:examples} for a real-world example)
need to be applied before an analyst can begin to enjoy
an orderly and meaningful data frame, matrix, or spreadsheet  being finally
at their disposal.
Amongst them we may find: string concatenation,
substring extraction, collation, sorting, Unicode normalisation,
transliteration, pattern matching, and date-time parsing.
Activities related to information retrieval, computer vision,
bioinformatics, natural language processing, or even musicology
can also benefit from including them
in the data processing pipelines, see \citep{speechlangproc,genome}.






% sentiment of emojis 10.1371/journal.pone.0144296



Base \proglang{R} \citep{Rproject:home} provides a few functions for dealing
with character strings, see \citep[Chapter~8]{Chambers2008:SoftDA}
and Table~\ref{Tab:oldstringr}.
However, it is the \pkg{stringr}  package \citep{Wickham2010:stringr},
first released in November 2009, that marks the first milestone of
implementing
the idea of a ``tidier'' API for text data processing.
In version 0.6.2 (dated 2012--12--06)
of \pkg{stringr}'s \code{README}, we read that this package:
\begin{quote}\it
\begin{itemize}
\item processes factors and characters in the same way,
\item gives functions consistent names and arguments,
\item simplifies string operations by eliminating options that you
don't need 95\% of the time,
\item produces outputs than can easily be used as inputs.
This includes ensuring that missing inputs result in missing outputs,
and zero length inputs result in zero length outputs,
\item completes \proglang{R}'s string handling functions with
useful functions from other programming languages.
\end{itemize}
\end{quote}
The list of the functions available in \pkg{stringr} at that time
is given in Table~\ref{Tab:oldstringr}.




\begin{table}[t!]
\centering

\begin{tabularx}{1.0\linewidth}{p{4.8cm}p{2.5cm}X}
\toprule
\pkg{stringr}                                  & \bfseries Base \proglang{R}            & \bfseries Purpose \\
\midrule
\code{str\_c()}                                & \code{paste()}                         & join strings \\                                                          \midrule
\code{str\_count()}                            & \code{gregexpr()}                      & count pattern matches\\                                                  \midrule
\code{str\_detect()}                           & \code{grepl()}                         & detect pattern matches \\                                                \midrule
\code{str\_dup()}                              &                                        & duplicate strings\\                                                      \midrule
\code{str\_extract()}, \code{str\_extract\_all()}  &                                    & extract (first, all) pattern matches  \\                                 \midrule
\code{str\_length()}                           & \code{nchar()}                         & compute string length \\                                                 \midrule
\code{str\_locate()}, \code{str\_locate\_all()}& \code{regexpr()}, \code{gregexpr()}    & locate (first, all) pattern matches \\                                   \midrule
\code{str\_match()}, \code{str\_match\_all()}   & \code{regexec()}                      & extract (first, all) matches to regex capture groups \\                  \midrule
\code{str\_pad()}                              &                                        & add whitespaces at beginning or end\\                                    \midrule
\code{str\_trim()}                             &                                        & remove whitespaces from beginning or end \\                              \midrule
\code{str\_replace()}, \code{str\_replace\_all()}   & \code{sub()}, \code{gsub()}       & replace (first, all) pattern matches with a replacement string\\        \midrule
\code{str\_split()}, \code{str\_split\_fixed()}&                                        & split up a string into  pieces \\                       \midrule
\code{str\_sub()}, \code{`{}str\_sub<-`(){}}   & \code{substring()}                     & extract or replace substrings\\                                         \midrule
\code{str\_wrap()}                             & \code{strwrap()}                       & split strings into text lines \\                                         \midrule
\code{word()}                                  &                                        & extract words from a sentence \\
\bottomrule
\end{tabularx}

\caption{\label{Tab:oldstringr} Functions in (the historical)
\pkg{stringr} 0.6.2 and their base \proglang{R} counterparts.}
\end{table}


Nevertheless, \pkg{stringr} was developed as a set of wrappers
around its base \proglang{R} counterparts, which not only limited its scope
but also could cause many portability issues. In particular, the same code
may yield different results on different operating systems, some services
such as the processing of particular languages may be unavailable
whatsoever, and so forth. For instance, varied variants of the
\pkg{PCRE} (versions 8.x or 10.x thereof)
pattern matching libraries may be linked to
during compilation. On Windows, there is a custom implementation
of \pkg{iconv} that has a set of character encoding
IDs not fully compatible with that on GNU/Linux: to select
the Polish locale, we are required to pass \strq{Polish\_Poland}
to \code{Sys.setlocale()} on Windows whereas \strq{pl\_PL} on
Linux.
Moreover, \proglang{R} can be build against the system \pkg{ICU}
so that it uses its Collator for comparing strings, however this is only
optional.


%
% A call to \code{extSoftVersion()} reports on the versions of
% third-party software  used by \proglang{R}.
% some resources might not be unavailable/not installed
% resources (e.g., locales) may be fetched by means of different IDs
% not necessarily the fastest
% %                      PCRE                       ICU                       TRE                     iconv
% %        "10.34 2019-11-21"                    "66.1" "TRE 0.8.0 R_fixes (BSD)"              "glibc 2.31"
% %
% %        PCRE                       ICU     TRE                        iconv
% % "8.42 2018-03-20"          "55.1"  "TRE 0.8.0 R_fixes (BSD)"  "win_iconv"




For example, let us consider the matching
of ``all letters'' by means of the built-in \code{gregexpr()} function
and the \pkg{TRE} (\code{perl=FALSE})
and \pkg{PCRE} (\code{perl=TRUE}) libraries
using a POSIX-like and Unicode-style character set
(see Section~\ref{Sec:regex} for more details):

\begin{Schunk}
\begin{Sinput}
R> library("stringi")  # substring extraction with stri_sub(), see below
R> x <- "AEZaezĄĘŻąęż"
R> stri_sub(x, gregexpr("[[:alpha:]]", x, perl=FALSE)[[1]], length=1)
R> stri_sub(x, gregexpr("[[:alpha:]]", x, perl=TRUE)[[1]],  length=1)
R> stri_sub(x, gregexpr("\\p{L}", x, perl=TRUE)[[1]],       length=1)
\end{Sinput}
\end{Schunk}

On Ubuntu Linux 20.04 (UTF-8 locale), the respective outputs are:

\begin{Schunk}
\begin{Soutput}
[1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
[1] "A" "E" "Z" "a" "e" "z"
[1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
\end{Soutput}
\end{Schunk}

On Windows, when \code{x} is marked as UTF-8, we get:

\begin{Schunk}
\begin{Soutput}
[1] "A" "E" "Z" "a" "e" "z"
[1] "A" "E" "Z" "a" "e" "z"
[1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
\end{Soutput}
\end{Schunk}

And again on Windows using the Polish locale
but \code{x} marked as natively-encoded (CP-1250 in this case):

\begin{Schunk}
\begin{Soutput}
[1] "A" "E" "Z" "a" "e" "z" "Ę" "ę"
[1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
[1] "A" "E" "Z" "a" "e" "z" "Ę" "ę"
\end{Soutput}
\end{Schunk}



In order to overcome such portability problems, in 2013 I have developed
the \pkg{stringi} package (pronounced ``stringy'',
IPA \textipa{[{stringi}]}).
Its API has been designed so as to be compatible and consistent with that
of \pkg{stringr}'s, which has already proven effective and convenient.
All the functions have been written from scratch to
guarantee that they are as efficient as possible.
For the processing of text in different languages,
which are plentiful, the \pkg{ICU} library
(see \url{http://site.icu-project.org/}) is relied upon to assure
full conformance to the Unicode standards.

Over the years, \pkg{stringi}
confirmed itself as robust,  production quality software.
Interestingly, from version 1.0, \pkg{stringr} has been rewritten as a
set of wrappers around \pkg{stringi} instead of base \proglang{R} routines;
% \pkg{stringr} currently exports length(ls("package:stringr")) objects
it aims to be more beginner-friendly,
see \cite[Chapter~14]{GrolemundWickham2017:rdatascience}.
On the other hand, \pkg{stringi} provides many more
functions (250
vs.~52); some of them are
more specialised or equipped with more control parameters
to enable fine-tuning.
This paper describes the facilities
provided by \pkg{stringi} in-depth so that the package's users
can get the most out of them. It also demonstrates the wide range
of tools that more advanced statisticians and data scientists may find
useful in their daily activities.

% confirmed itself as robust,  production quality software
% yet, new features being added $\sim$ biyearly













% <<>>=
% oldlocale <- Sys.getlocale("LC_COLLATE")
% Sys.setlocale("LC_COLLATE", "sk_SK")
% sort(c("hladný", "chladný"))
% Sys.setlocale("LC_COLLATE", oldlocale)
% @
%
%
% % Warning message:
% % In Sys.setlocale("LC_COLLATE", "sk_SK") :
% %   OS reports request to set locale to "sk_SK" cannot be honored
% %
%
%
% This code works on my Linux, but is not portable. It's because:
%
%     Other Linux users may not have Slovak rule-base installed (and not everyone has abilities to do it on his/her own).
%     Windows users don't use BCP 47-based locale names. There, LCID \code{Slovak_Slovakia.1250} is appropriate.
%
% And so on.





% x <- "AEZaezĄĘŻąęż"
% stri_sub(x, gregexpr("[[:alpha:]]", x, perl=FALSE)[[1]], length=1)
% #stri_sub(x, gregexpr("\\p{L}", x, perl=FALSE)[[1]], length=1)
% stri_sub(x, gregexpr("[[:alpha:]]", x, perl=TRUE)[[1]], length=1)
% stri_sub(x, gregexpr("\\p{L}", x, perl=TRUE)[[1]], length=1)
% stri_extract_all_regex(x, "[[:alpha:]]")[[1]]
% stri_extract_all_regex(x, "\\p{L}")[[1]]
%
%
% Linux (UTF-8 locale):
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
% [1] "A" "E" "Z" "a" "e" "z"
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"

% Windows (UTF-8)
%
%  stri_sub(x, gregexpr("[[:alpha:]]", x, perl=FALSE)[[1]], length=1)
% [1] "A" "E" "Z" "a" "e" "z"
% > #stri_sub(x, gregexpr("\\p{L}", x, perl=FALSE)[[1]], length=1)
% > stri_sub(x, gregexpr("[[:alpha:]]", x, perl=TRUE)[[1]], length=1)
% [1] "A" "E" "Z" "a" "e" "z"
% > stri_sub(x, gregexpr("\\p{L}", x, perl=TRUE)[[1]], length=1)
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
% > stri_extract_all_regex(x, "[[:alpha:]]")[[1]]
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
% > stri_extract_all_regex(x, "\\p{L}")[[1]]
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"

%
% > stri_sub(x, gregexpr("[[:alpha:]]", x, perl=FALSE)[[1]], length=1)
% [1] "A" "E" "Z" "a" "e" "z" "Ę" "ę"
% > #stri_sub(x, gregexpr("\\p{L}", x, perl=FALSE)[[1]], length=1)
% > stri_sub(x, gregexpr("[[:alpha:]]", x, perl=TRUE)[[1]], length=1)
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
% > stri_sub(x, gregexpr("\\p{L}", x, perl=TRUE)[[1]], length=1)
% [1] "A" "E" "Z" "a" "e" "z" "Ę" "ę"
% > stri_extract_all_regex(x, "[[:alpha:]]")[[1]]
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"
% > stri_extract_all_regex(x, "\\p{L}")[[1]]
%  [1] "A" "E" "Z" "a" "e" "z" "Ą" "Ę" "Ż" "ą" "ę" "ż"





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\medskip
What remains is set out as follows.
Section~\ref{Sec:examples} gives some motivational examples
that illustrate the importance of string processing in data preparation
activities.
Basic string operations such as substring extraction and concatenation
are discussed in Section~\ref{Sec:basic}.
Section~\ref{Sec:fixed} discusses searching for fixed substrings.
Section~\ref{Sec:regex} details pattern matching with \pkg{ICU} regular
expressions.
Section~\ref{Sec:collator} deals with the locale-aware \pkg{ICU} Collator,
suitable for natural language processing activities.
Section~\ref{Sec:other} introduces other operations such as text boundary
analysis
or date-time formatting and parsing.
Section~\ref{Sec:io} details on encoding conversion and detection
as well as Unicode normalisation.
Finally, Section~\ref{Sec:conclusions} concludes the paper.

All the code chunks' outputs presented in this paper were obtained using
\proglang{R}~4.0.2.
The \proglang{R} environment itself and all the packages used herein
are available from CRAN at \url{https://CRAN.R-project.org/}.

\begin{Schunk}
\begin{Sinput}
R> # install.packages("stringi")  # to download from CRAN and install
R> library("stringi")  # load and attach the package's namespace
\end{Sinput}
\end{Schunk}

\noindent
Here we describe \pkg{stringi} 1.5.3,
which has been built against the following version of the \pkg{ICU} library:
%Moreover, for \pkg{stringr}~packageVersion("stringr")


%%% # R CMD INSTALL stringi --preclean --configure-args='--disable-pkg-config'

\begin{Schunk}
\begin{Sinput}
R> cat(stri_info(short=TRUE))
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Soutput}
stringi_1.5.3 (en_AU.UTF-8; ICU4C 61.1 [bundle]; Unicode 10.0)
\end{Soutput}
\end{Schunk}


\noindent
\pkg{stringi} is an open source project
distributed under the terms of the BSD-3-clause license.
Its most recent development snapshot is available through GitHub at
\url{https://github.com/gagolews/stringi}. The bug- and feature request
tracker can be accessed from
\url{https://github.com/gagolews/stringi/issues}.
Moreover, its homepage -- which includes a detailed documentation
of the package's API -- is located at \url{https://stringi.gagolewski.com/}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivational example: Data preparation}\label{Sec:examples}

Before going into technical details on the \pkg{stringi} package itself,
let us first demonstrate that string processing is indeed a relevant
part of the statistical analysis workflow.
What follows is a short case study where we prepare a web-scraped
data set for further processing.

Assume we wish to gather and analyse
climate data for major cities around the world based on information
downloaded from \textit{Wikipedia}.
For each location from a given list of settlements (e.g.,
gathered from one of the pages linked under
\url{https://en.wikipedia.org/wiki/Lists_of_cities}),
we would like to harvest the relevant temperature and precipitation data.
Without harm in generality, let us focus on the city of Melbourne,
VIC, Australia.

The parsing of the city's~\textit{Wikipedia} page
can be done by means of the functions from the
\pkg{xml2} \citep{xml2}
and \pkg{rvest} \citep{rvest} packages.

\begin{Schunk}
\begin{Sinput}
R> library("xml2")
R> library("rvest")
\end{Sinput}
\end{Schunk}

First, we load and parse the downloaded \proglang{HTML} file.

\begin{Schunk}
\begin{Sinput}
R> # downloaded from https://en.wikipedia.org/wiki/Melbourne on 2020-09-17,
R> # see https://github.com/gagolews/stringi/tree/master/datasets
R> f <- read_html("20200917_wikipedia_melbourne.html")
\end{Sinput}
\end{Schunk}

Second, we extract all \code{<table>} elements and gather them
in a list of \proglang{HTML} nodes, \code{all\_tables}.
We then extract the underlying raw text data and store them in a
character vector named \code{text\_tables}.

\begin{Schunk}
\begin{Sinput}
R> all_tables <- html_nodes(f, "table")
R> text_tables <- sapply(all_tables, html_text)
R> str(text_tables, nchar.max=65, vec.len=5, strict.width="wrap") # preview
\end{Sinput}
\begin{Soutput}
chr [1:45] "MelbourneVictoriaFrom top, left to right: Flinde"| __truncated__
   "Mean max temp\n Mean min temp\n Annual rainfal"| __truncated__ "This
   section needs additional citations for veri"| __truncated__ "Climate data
   for Melbourne Regional Office (1991"| __truncated__ "Country of Birth
   (2016)[178]Birthplace[N 1]\nPop"| __truncated__ ...
\end{Soutput}
\end{Schunk}

Most~\textit{Wikipedia} pages related to particular cities
include a table labelled as ``Climate data''.
We need to pinpoint it amongst all the other tables.
For this, we will rely on \pkg{stringi}'s
\code{stri\_detect\_} \code{fixed()} function that, in the configuration
below, is used to extract the index of the relevant table.

\begin{Schunk}
\begin{Sinput}
R> library("stringi")
R> (idx <- which(stringi::stri_detect_fixed(text_tables, "climate data",
+    case_insensitive=TRUE, max_count=1)))
\end{Sinput}
\begin{Soutput}
[1] 4
\end{Soutput}
\end{Schunk}

Of course, the detailed description of all the facilities
brought by \pkg{stringi} is covered in the sequel.
In the meantime, let us use \pkg{rvest}'s \code{html\_table()}
to convert the above table to a data frame object.


\begin{Schunk}
\begin{Sinput}
R> (x <- html_table(all_tables[[idx]], fill=TRUE))
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Soutput}
   Climate data for Melbourne Regional Office (1991–2015)
1                                                   Month
2                                     Record high °C (°F)
3                                    Average high °C (°F)
4                                      Daily mean °C (°F)
5                                     Average low °C (°F)
6                                      Record low °C (°F)
7                            Average rainfall mm (inches)
8                              Average rainy days (≥ 1mm)
9                 Average afternoon relative humidity (%)
10                            Mean monthly sunshine hours
11             Source: Bureau of Meteorology.[85][86][87]
   Climate data for Melbourne Regional Office (1991–2015).1 ...
1                                                       Jan ...
2                                               45.6(114.1) ...
3                                                27.0(80.6) ...
4                                                21.6(70.9) ...
5                                                16.1(61.0) ...
6                                                 5.5(41.9) ...
7                                                44.2(1.74) ...
8                                                       5.6 ...
9                                                        47 ...
10                                                      279 ...
11               Source: Bureau of Meteorology.[85][86][87] ...
   Climate data for Melbourne Regional Office (1991–2015).3
1                                                      Year
2                                               46.4(115.5)
3                                                20.8(69.4)
4                                                16.2(61.2)
5                                                11.6(52.9)
6                                                −2.8(27.0)
7                                              600.9(23.66)
8                                                      90.6
9                                                        51
10                                                    2,191
11               Source: Bureau of Meteorology.[85][86][87]
\end{Soutput}
\end{Schunk}

It is evident that this object requires some significant
cleansing and transforming before it can be subject
to any statistical analyses.
First, for the sake of convenience, let us convert it
to a character matrix so that the processing of all cells can
be vectorised
(a matrix in \proglang{R} is just a single ``long'' vector,
whereas a data frame is a list of many atomic vectors).

\begin{Schunk}
\begin{Sinput}
R> x <- as.matrix(x)
\end{Sinput}
\end{Schunk}

The \code{as.numeric()} function (which we would soon like to use)
may find parsing the Unicode MINUS SIGN (U+2212, ``−'') difficult,
therefore let us call the transliterator to replace it
(and other potentially problematic characters) with its simpler equivalent:

\begin{Schunk}
\begin{Sinput}
R> x[, ] <- stri_trans_general(x, "Publishing-Any; Any-ASCII")
\end{Sinput}
\end{Schunk}

Note that it is the first row that in fact gives the column names.
Moreover, the last row just gives the data source and hence may be removed.

\begin{Schunk}
\begin{Sinput}
R> dimnames(x) <- list(x[, 1], x[1, ])  # row, column names
R> x <- x[2:(nrow(x)-1), 2:ncol(x)]     # skip 1st/last row and 1st column
R> x[, c(1, ncol(x))]  # example columns
\end{Sinput}
\begin{Soutput}
                                        Jan           Year
Record high °C (°F)                     "45.6(114.1)" "46.4(115.5)"
Average high °C (°F)                    "27.0(80.6)"  "20.8(69.4)"
Daily mean °C (°F)                      "21.6(70.9)"  "16.2(61.2)"
Average low °C (°F)                     "16.1(61.0)"  "11.6(52.9)"
Record low °C (°F)                      "5.5(41.9)"   "-2.8(27.0)"
Average rainfall mm (inches)            "44.2(1.74)"  "600.9(23.66)"
Average rainy days (>= 1mm)             "5.6"         "90.6"
Average afternoon relative humidity (%) "47"          "51"
Mean monthly sunshine hours             "279"         "2,191"
\end{Soutput}
\end{Schunk}

Commas that are used as thousands separators (commas that are surrounded
by digits) should be dropped:

\begin{Schunk}
\begin{Sinput}
R> x[, ] <- stri_replace_all_regex(x, "(?<=\\d),(?=\\d)", "")
\end{Sinput}
\end{Schunk}

The numbers and alternative units in parentheses are redundant,
therefore these should be taken care of as well:

\begin{Schunk}
\begin{Sinput}
R> x[, ] <- stri_replace_all_regex(x,
+    "(\\d+(?:\\.\\d+)?)\\(\\d+(?:\\.\\d+)?\\)", "$1")
R> dimnames(x)[[1]] <- stri_replace_all_fixed(dimnames(x)[[1]],
+    c(" (°F)", " (inches)"), c("", ""), vectorise_all=FALSE)
\end{Sinput}
\end{Schunk}

At last, \code{as.numeric()} can be used to interpret the all the strings
as numbers:

\begin{Schunk}
\begin{Sinput}
R> x <- structure(as.numeric(x), dim=dim(x), dimnames=dimnames(x))
R> x[, c(1, 6, ncol(x))]  # example columns
\end{Sinput}
\begin{Soutput}
                                          Jan   Jun   Year
Record high °C                           45.6  22.4   46.4
Average high °C                          27.0  15.1   20.8
Daily mean °C                            21.6  11.7   16.2
Average low °C                           16.1   8.2   11.6
Record low °C                             5.5  -2.2   -2.8
Average rainfall mm                      44.2  49.5  600.9
Average rainy days (>= 1mm)               5.6   8.6   90.6
Average afternoon relative humidity (%)  47.0  61.0   51.0
Mean monthly sunshine hours             279.0 108.0 2191.0
\end{Soutput}
\end{Schunk}

We now have a cleansed matrix at our disposal.
We can, for instance, compute the monthly temperature ranges:

\begin{Schunk}
\begin{Sinput}
R> x["Record high °C", -ncol(x)]-x["Record low °C", -ncol(x)]
\end{Sinput}
\begin{Soutput}
 Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
40.1 41.9 38.9 33.4 29.8 24.6 26.1 28.6 31.9 36.8 38.4 39.3
\end{Soutput}
\end{Schunk}

\noindent
or average daily precipitation:

\begin{Schunk}
\begin{Sinput}
R> sum(x["Average rainfall mm", -ncol(x)]) / 365.25
\end{Sinput}
\begin{Soutput}
[1] 1.6463
\end{Soutput}
\end{Schunk}

\noindent
and so forth.

For the climate data on other cities, very similar steps are
needed -- the whole process of scraping and cleansing data
can quite easily be automatised, perhaps with some minor adjustments.
The above functions are not only convenient to use, but also efficient
and portable across different platforms.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Basic string operations}\label{Sec:basic}

Let us proceed with a detailed description of the most important
facilities in the \pkg{stringi} package.
First and foremost, we should emphasise that the  \proglang{R} language
itself does not provide access to any classical scalar types.
Individual character strings are  wrapped around atomic vectors
of type \code{character}:

\begin{Schunk}
\begin{Sinput}
R> "spam"          # or 'spam'
\end{Sinput}
\begin{Soutput}
[1] "spam"
\end{Soutput}
\begin{Sinput}
R> typeof("spam")
\end{Sinput}
\begin{Soutput}
[1] "character"
\end{Soutput}
\begin{Sinput}
R> length("spam")  # a character vector of length 1 - a single string
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}

\noindent
Not having a separate scalar type is quite convenient
from the practical side; the so-called
\emph{vectorisation} strategy encourages writing of code for processing
\textit{whole} collections of objects all at once, regardless of their size.
For example, let's consider the following data frame:


\begin{Schunk}
\begin{Sinput}
R> # see https://github.com/gagolews/stringi/tree/master/datasets
R> (birth_dates <-
+    head(read.csv(header=TRUE, comment="#", file="birth_dates.csv"), n=3))
\end{Sinput}
\begin{Soutput}
               Name  BirthDate
1 Eckehard Grünfeld 01.11.1911
2     Zbyněk Slavík 29.03.1966
3      Marx Crowell 17.03.2009
\end{Soutput}
\end{Schunk}

\noindent
The \code{Name} column is of type \code{character}.
For instance (with all the details provided in a section to follow),
here is how we can separate the first and the last names from
each other (assuming for simplicity that no middle names are given),
using just a single function call:

\begin{Schunk}
\begin{Sinput}
R> (birth_names <- stri_split_fixed(birth_dates$Name, " ",
+    n=2, simplify=TRUE))
\end{Sinput}
\begin{Soutput}
     [,1]       [,2]
[1,] "Eckehard" "Grünfeld"
[2,] "Zbyněk"   "Slavík"
[3,] "Marx"     "Crowell"
\end{Soutput}
\end{Schunk}

\noindent
Due to {vectorisation}, we can generally
avoid using \code{for}/\code{while} loops,
which makes the code much more readable, maintainable, and faster to execute.





\subsection{Computing length and width}


First we shall review the functions related to counting
the number of \textit{entities} in each string.



\paragraph{Length.}
Let's consider the following character vector:

\begin{Schunk}
\begin{Sinput}
R> x <- c("spam", "bacon", "", "sausage", NA, "spam")
R> length(x)        # vector length
\end{Sinput}
\begin{Soutput}
[1] 6
\end{Soutput}
\end{Schunk}


\code{stri_length()} computes the \textit{length}
of each string.
More precisely,
the function gives the number of Unicode code points in each string,
see Section~\ref{Sec:codepoints} for more details.

\begin{Schunk}
\begin{Sinput}
R> stri_length(x)
\end{Sinput}
\begin{Soutput}
[1]  4  5  0  7 NA  4
\end{Soutput}
\end{Schunk}

\noindent
\code{stri_length(x)} returns a numeric vector \code{l},
with the same number of elements as \code{x}, such that, for every \code{i},
\code{l[i]} is the length of the string \code{x[i]}.
Note that the 3rd element in \code{x} is an empty string, \strq{},
hence its length is  0.
Moreover, there is a missing (\code{NA}) value
at index 5, so the corresponding length is \textit{undefined} as well.
\pkg{stringi} applies this rule consistently across all its functions.



\paragraph{Zero-length.}
To quickly determine which of the items are empty strings, we may call:

\begin{Schunk}
\begin{Sinput}
R> stri_isempty(x)
\end{Sinput}
\begin{Soutput}
[1] FALSE FALSE  TRUE FALSE    NA FALSE
\end{Soutput}
\end{Schunk}

\noindent
Note that we distinguish between an empty character vector
(\code{character(0)}, i.e., a zero-length sequence)
and an empty string, i.e., a string of length 0 wrapped in a vector
of length~1.

\begin{Schunk}
\begin{Sinput}
R> length(character(0))      # no strings at all
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\begin{Sinput}
R> stri_length(character(0))
\end{Sinput}
\begin{Soutput}
integer(0)
\end{Soutput}
\begin{Sinput}
R> length("")                # one empty string
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\begin{Sinput}
R> stri_length("")
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\end{Schunk}


\paragraph{Width.}
Sometimes merely knowing  the number of characters in a string is not enough.
For instance, when preparing a formatted output (e.g., in an automatically
generated report),
a string's \textit{width} estimate -- an approximate number of text columns
it occupies when printed using a mono-spaced font -- may be more informative.
In particular, many CJK (Chinese, Japanese, Korean) or emoji
characters take up two text cells. Some code points, on the other hand,
are of width 0 (e.g., the ZERO WIDTH SPACE, U+200B).



% \begin{CJK*}{UTF8}{bsmi}
\begin{Schunk}
\begin{Sinput}
R> stri_length(c("你好", "\u200b\u200b\u200b\u200b", "123456"))
\end{Sinput}
\begin{Soutput}
[1] 2 4 6
\end{Soutput}
\end{Schunk}
% \end{CJK*}

\noindent
The first string (a greeting) consists of 2 Chinese characters
(U+4F60, U+597D),
the second is comprised of 4 zero-width spaces, and the third one
carries 6 ASCII digits. Here are their corresponding widths:

% \begin{CJK*}{UTF8}{bsmi}
\begin{Schunk}
\begin{Sinput}
R> stri_width(c("你好", "\u200b\u200b\u200b\u200b", "123456"))
\end{Sinput}
\begin{Soutput}
[1] 4 0 6
\end{Soutput}
\end{Schunk}
% \end{CJK*}






% SKIP: stri_numbytes






\subsection{Joining}

Below we describe the functions that are based on string concatenation.


\paragraph{Operator \code{\%s+\%}.}
To join (concatenate) the corresponding strings in two character vectors,
we may use the binary \code{\%s+\%} operator:

\begin{Schunk}
\begin{Sinput}
R> "tasty" %s+% "spam"
\end{Sinput}
\begin{Soutput}
[1] "tastyspam"
\end{Soutput}
\begin{Sinput}
R> c("tasty", "delicious") %s+% c("spam", "bacon")  # elementwise
\end{Sinput}
\begin{Soutput}
[1] "tastyspam"      "deliciousbacon"
\end{Soutput}
\begin{Sinput}
R> c("tasty", "delicious") %s+% "spam"              # recycling rule
\end{Sinput}
\begin{Soutput}
[1] "tastyspam"     "deliciousspam"
\end{Soutput}
\begin{Sinput}
R> c("tasty", "delicious", "savoury", "yummy") %s+% c("spam", "bacon")
\end{Sinput}
\begin{Soutput}
[1] "tastyspam"      "deliciousbacon" "savouryspam"    "yummybacon"
\end{Soutput}
\begin{Sinput}
R> character(0) %s+% c("spam", "bacon")
\end{Sinput}
\begin{Soutput}
character(0)
\end{Soutput}
\end{Schunk}

This operator is vectorised in exactly the same manner
as other arithmetic operators in \proglang{R}.
In particular, the recycling rule is used if the inputs are of different
lengths and if one of the arguments is empty, the result is a zero-length
vector as well. Moreover, \pkg{stringi} \textit{does} enforce the consistent
propagation of missing values
(unlike in the case of the built-in \code{paste()}
function):

\begin{Schunk}
\begin{Sinput}
R> x %s+% "!"
\end{Sinput}
\begin{Soutput}
[1] "spam!"    "bacon!"   "!"        "sausage!" NA         "spam!"
\end{Soutput}
\end{Schunk}

\noindent
For dealing with missing values, we may use convenience functions such as
\code{stri_replace_na()}, \code{stri_omit_na()},
and
 if additionally we would like to get rid
of empty strings in a vector,
\code{stri_omit_empty_na()}:


\begin{Schunk}
\begin{Sinput}
R> stri_replace_na(x, "<NA>") %s+% "!"
\end{Sinput}
\begin{Soutput}
[1] "spam!"    "bacon!"   "!"        "sausage!" "<NA>!"    "spam!"
\end{Soutput}
\begin{Sinput}
R> stri_omit_empty_na(x) %s+% "!"
\end{Sinput}
\begin{Soutput}
[1] "spam!"    "bacon!"   "sausage!" "spam!"
\end{Soutput}
\end{Schunk}


\paragraph{Flattening.}
The elements in a character vector can be joined altogether to
form a single string via a call to  \code{stri\_flatten()}:

\begin{Schunk}
\begin{Sinput}
R> stri_flatten(stri_omit_empty_na(x))  # collapse="" by default
\end{Sinput}
\begin{Soutput}
[1] "spambaconsausagespam"
\end{Soutput}
\begin{Sinput}
R> stri_flatten(stri_omit_empty_na(x), collapse=", ")
\end{Sinput}
\begin{Soutput}
[1] "spam, bacon, sausage, spam"
\end{Soutput}
\end{Schunk}


\paragraph{Generalisation.}
Both the \code{\%s+\%} operator and the \code{stri\_flatten()} function
are generalised by
\code{stri\_join()} (alias: \code{stri\_paste()}, \code{stri\_c()}):

\begin{Schunk}
\begin{Sinput}
R> stri_join(c("X", "Y", "Z"), 1:6, "!")  # sep="", collapse=NULL
\end{Sinput}
\begin{Soutput}
[1] "X1!" "Y2!" "Z3!" "X4!" "Y5!" "Z6!"
\end{Soutput}
\begin{Sinput}
R> stri_join(c("X", "Y", "Z"), 1:6, "+", sep=".", collapse="; ")
\end{Sinput}
\begin{Soutput}
[1] "X.1.+; Y.2.+; Z.3.+; X.4.+; Y.5.+; Z.6.+"
\end{Soutput}
\end{Schunk}


\noindent
Note how the two (1st, 3rd) shorter vectors were {recycled} to match
the longest vector's (2nd) length. The latter was of numeric type,
but it was implicitly coerced with  a call to \code{as.character()}.
More examples:

\begin{Schunk}
\begin{Sinput}
R> stri_join(birth_names[,2], birth_names[,1], sep=", ")
\end{Sinput}
\begin{Soutput}
[1] "Grünfeld, Eckehard" "Slavík, Zbyněk"     "Crowell, Marx"
\end{Soutput}
\begin{Sinput}
R> outer(LETTERS[1:3], 1:5, stri_join, sep=".") # outer product
\end{Sinput}
\begin{Soutput}
     [,1]  [,2]  [,3]  [,4]  [,5]
[1,] "A.1" "A.2" "A.3" "A.4" "A.5"
[2,] "B.1" "B.2" "B.3" "B.4" "B.5"
[3,] "C.1" "C.2" "C.3" "C.4" "C.5"
\end{Soutput}
\end{Schunk}


\paragraph{Duplicating.}
To duplicate given strings, call
\code{stri\_dup()} or the \code{\%s*\%} operator:


\begin{Schunk}
\begin{Sinput}
R> stri_dup(letters[1:5], 2)
\end{Sinput}
\begin{Soutput}
[1] "aa" "bb" "cc" "dd" "ee"
\end{Soutput}
\begin{Sinput}
R> stri_dup("spam", 1:3)
\end{Sinput}
\begin{Soutput}
[1] "spam"         "spamspam"     "spamspamspam"
\end{Soutput}
\begin{Sinput}
R> stri_dup(letters[1:3], 1:3)
\end{Sinput}
\begin{Soutput}
[1] "a"   "bb"  "ccc"
\end{Soutput}
\begin{Sinput}
R> "a" %s*% 5
\end{Sinput}
\begin{Soutput}
[1] "aaaaa"
\end{Soutput}
\end{Schunk}

\noindent
Again, we see a vectorisation with regards to all the arguments.



\paragraph{Within-list joining.}
There is also a convenience function that applies \code{stri\_flatten()}
on each character vector in a given list:

\begin{Schunk}
\begin{Sinput}
R> words <- list(c("spam", "bacon", "sausage", "spam"), c("eggs", "spam"))
R> stri_join_list(words, sep=",")
\end{Sinput}
\begin{Soutput}
[1] "spam,bacon,sausage,spam" "eggs,spam"
\end{Soutput}
\begin{Sinput}
R> stri_join_list(words, sep=",", collapse="; ")
\end{Sinput}
\begin{Soutput}
[1] "spam,bacon,sausage,spam; eggs,spam"
\end{Soutput}
\end{Schunk}

\noindent
We shall see that such lists of strings
are generated by \code{stri_sub_all()}, \code{stri_extract_all()},
and similar functions.





\subsection{Extracting and replacing substrings}

The next group of functions deals with the extraction and replacement
of particular sequences of code points in given strings.

\paragraph{Indexing vectors.}
In order to pick a subsequence from any \proglang{R} vector,
we use the square-bracket operator\footnote{More precisely, \code{x[i]}
is a syntactic sugar for a call to \code{`[`(x, i)}.
Moreover, if \code{x} is a list, \code{x[[i]]} can be used to
extract its \code{i}-th element (alias \code{`[[`(x, i)}).
Knowing the ``functional'' form of the operators allows us to, for instance,
extract all first elements from each vector in a list
by simply calling \code{sapply(x, "[[", 1)}.}
with an index vector consisting of either
non-negative integers, negative integers,
or logical values\footnote{If an object's \code{names} attribute is set,
indexing with a character vector is also possible.}.

\begin{Schunk}
\begin{Sinput}
R> x[1:3] # from 1st to 3rd string
\end{Sinput}
\begin{Soutput}
[1] "spam"  "bacon" ""
\end{Soutput}
\begin{Sinput}
R> x[c(1, length(x))] # 1st and last
\end{Sinput}
\begin{Soutput}
[1] "spam" "spam"
\end{Soutput}
\begin{Sinput}
R> x[-1]  # all but 1st
\end{Sinput}
\begin{Soutput}
[1] "bacon"   ""        "sausage" NA        "spam"
\end{Soutput}
\begin{Sinput}
R> x[!stri_isempty(x) & !is.na(x)] # filtering based on a logical vector
\end{Sinput}
\begin{Soutput}
[1] "spam"    "bacon"   "sausage" "spam"
\end{Soutput}
\end{Schunk}

\paragraph{Extracting substrings.}
A character vector is, in its very own essence, a sequence of
sequences of code points.
To extract specific substrings from each string in a collection,
we can use the \code{stri\_sub()} function.

\begin{Schunk}
\begin{Sinput}
R> y <- "spam, egg, spam, spam, bacon, and spam"
R> stri_sub(y, 18)           # from 18th code point to end
\end{Sinput}
\begin{Soutput}
[1] "spam, bacon, and spam"
\end{Soutput}
\begin{Sinput}
R> stri_sub(y, 12, to=15)    # from 12th to 15th code point  (inclusive)
\end{Sinput}
\begin{Soutput}
[1] "spam"
\end{Soutput}
\begin{Sinput}
R> stri_sub(y, 12, length=4) # 4 code points from 12th
\end{Sinput}
\begin{Soutput}
[1] "spam"
\end{Soutput}
\end{Schunk}

Moreover, negative indices count from the end of a string.

\begin{Schunk}
\begin{Sinput}
R> stri_sub(y, -15)           # from 15th last to end
\end{Sinput}
\begin{Soutput}
[1] "bacon, and spam"
\end{Soutput}
\end{Schunk}


\paragraph{\code{stri\_sub\_all()}.}
The \code{stri_sub()}  function is of course vectorised with respect
to all its arguments
(the character vector, \code{from}, and \code{to} or \code{length}).
If one of the vectors is of smaller length than the other ones,
the recycling rule is applied as usual. For instance:

\begin{Schunk}
\begin{Sinput}
R> stri_sub(y, c(1, 12, 18), length=4) # different substrings of one string
\end{Sinput}
\begin{Soutput}
[1] "spam" "spam" "spam"
\end{Soutput}
\begin{Sinput}
R> stri_sub(x[c(1, 2, 4)], from=-3)    # same substrings of different strings
\end{Sinput}
\begin{Soutput}
[1] "pam" "con" "age"
\end{Soutput}
\begin{Sinput}
R> stri_sub(x[c(1, 2, 4)],
+    c(-4, -2, -5))  # different substrings of different strings
\end{Sinput}
\begin{Soutput}
[1] "spam"  "on"    "usage"
\end{Soutput}
\end{Schunk}

If some deeper vectorisation level is necessary, \code{stri_sub_all()}
comes in handy. It allows to extract multiple (possibly different) substrings
from all the strings provided:


\begin{Schunk}
\begin{Sinput}
R> (z <- stri_sub_all(x[c(1, 2, 4)],
+    from=  list(c(1, 3, 4), -2, c(1, 4)),
+    length=list(1,           2, c(4, 3))))
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "s" "a" "m"

[[2]]
[1] "on"

[[3]]
[1] "saus" "sag"
\end{Soutput}
\end{Schunk}

As the number of substrings to extract from each string might vary,
the result is a list
of character strings. These may all be concatenated by means of the
above-mentioned \code{stri_join_list()} function.

\begin{Schunk}
\begin{Sinput}
R> stri_join_list(z, sep=", ")
\end{Sinput}
\begin{Soutput}
[1] "s, a, m"   "on"        "saus, sag"
\end{Soutput}
\end{Schunk}


On a side note, there is also a more flexible version
of the built-in \code{simplify2array()} function whose
aim is to convert such lists to matrices.

\begin{Schunk}
\begin{Sinput}
R> stri_list2matrix(z)
\end{Sinput}
\begin{Soutput}
     [,1] [,2] [,3]
[1,] "s"  "on" "saus"
[2,] "a"  NA   "sag"
[3,] "m"  NA   NA
\end{Soutput}
\begin{Sinput}
R> stri_list2matrix(z, byrow=TRUE, fill="", n_min=5)
\end{Sinput}
\begin{Soutput}
     [,1]   [,2]  [,3] [,4] [,5]
[1,] "s"    "a"   "m"  ""   ""
[2,] "on"   ""    ""   ""   ""
[3,] "saus" "sag" ""   ""   ""
\end{Soutput}
\end{Schunk}


Again, let's note that no explicit \code{for}/\code{while} loops are necessary.
For example, here is a way to extract non-consecutive substrings from each string --
ones that consist of the first and the last letter:

\begin{Schunk}
\begin{Sinput}
R> stri_join_list(stri_sub_all(x[c(1, 2, 4)], c(1, -1), length=1))
\end{Sinput}
\begin{Soutput}
[1] "sm" "bn" "se"
\end{Soutput}
\end{Schunk}


\paragraph{Permuting code points.}
Somehow related to the above are different ways to construct
various permutations (possibly with replacement) of code points in a string:

\begin{Schunk}
\begin{Sinput}
R> stri_join_list(stri_sub_all("spam", c(4, 3, 2, 3, 1), length=1))
\end{Sinput}
\begin{Soutput}
[1] "mapas"
\end{Soutput}
\begin{Sinput}
R> stri_rand_shuffle("bacon")  # random order
\end{Sinput}
\begin{Soutput}
[1] "anobc"
\end{Soutput}
\begin{Sinput}
R> stri_reverse("spam")        # reverse order
\end{Sinput}
\begin{Soutput}
[1] "maps"
\end{Soutput}
\end{Schunk}




\paragraph{\code{from\_to} matrices.}
The second parameter of both \code{stri_sub()} and \code{stri_sub_list()}
can also be fed with a two-column matrix
of the form \code{cbind(from, to)}. Here, the first column
gives the start indices and the second column defines the end ones.
Such matrices are generated, amongst others, by the \code{stri_locate_*()}
functions (see below for details).

\begin{Schunk}
\begin{Sinput}
R> (from_to <- cbind(from=c(1, 12, 18), to=c(4, 15, 21))) # +optional labels
\end{Sinput}
\begin{Soutput}
     from to
[1,]    1  4
[2,]   12 15
[3,]   18 21
\end{Soutput}
\begin{Sinput}
R> stri_sub(y, from_to)
\end{Sinput}
\begin{Soutput}
[1] "spam" "spam" "spam"
\end{Soutput}
\end{Schunk}


\noindent Another example (the recycling rule):

\begin{Schunk}
\begin{Sinput}
R> (from_to <- matrix(1:8, ncol=2, byrow=TRUE))
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,]    1    2
[2,]    3    4
[3,]    5    6
[4,]    7    8
\end{Soutput}
\begin{Sinput}
R> stri_sub(c("abcdefgh", "ijklmnop"), from_to)
\end{Sinput}
\begin{Soutput}
[1] "ab" "kl" "ef" "op"
\end{Soutput}
\end{Schunk}

\noindent
Now let's note the difference between the above output and the following one:

\begin{Schunk}
\begin{Sinput}
R> stri_sub_all(c("abcdefgh", "ijklmnop"), from_to)
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "ab" "cd" "ef" "gh"

[[2]]
[1] "ij" "kl" "mn" "op"
\end{Soutput}
\end{Schunk}




\paragraph{Replacing substrings.}
\code{stri\_sub\_replace()} returns a version
of a character vector with each specified chunk replaced with another
string:

\begin{Schunk}
\begin{Sinput}
R> stri_sub_replace(c("abcde", "ABCDE"),
+    from=c(2, 4), length=c(1, 2), replacement=c("X", "Y"))
\end{Sinput}
\begin{Soutput}
[1] "aXcde" "ABCY"
\end{Soutput}
\begin{Sinput}
R> stri_sub_replace("abcde",
+    from=c(2, 4), length=1, replacement=c("X", "Y"))
\end{Sinput}
\begin{Soutput}
[1] "aXcde" "abcYe"
\end{Soutput}
\end{Schunk}

\noindent
Similarly, \code{stri\_sub\_replace\_all()} allows for replacing
multiple substrings within each component of a character vector:

\begin{Schunk}
\begin{Sinput}
R> stri_sub_replace_all(c("abcde", "ABCDE"),
+    from=c(2, 4), length=1, replacement=c("X", "Y"))
\end{Sinput}
\begin{Soutput}
[1] "aXcYe" "AXCYE"
\end{Soutput}
\begin{Sinput}
R> stri_sub_replace_all("abcde",
+    from=c(2, 4), length=1, replacement=c("X", "Y"))
\end{Sinput}
\begin{Soutput}
[1] "aXcYe"
\end{Soutput}
\begin{Sinput}
R> stri_sub_replace_all(c("abcde", "ABCDE"),
+    from=list(c(2, 4), c(1, 3)), length=list(1, c(1, 2)),
+    replacement=list("Z", c("XX", "YYYYY")))
\end{Sinput}
\begin{Soutput}
[1] "aZcZe"     "XXBYYYYYE"
\end{Soutput}
\end{Schunk}



\paragraph{Replacing substrings in-place.}
The corresponding {replacement functions} allow for modifying
a character vector in-place:

\begin{Schunk}
\begin{Sinput}
R> y2 <- y
R> stri_sub(y2, 7, length=3) <- "spam"  # in-place replacement, egg → spam
R> print(y2)                            # y2 has been changed
\end{Sinput}
\begin{Soutput}
[1] "spam, spam, spam, spam, bacon, and spam"
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
R> y3 <- "a b c"
R> stri_sub_all(y3, c(1, 3, 5), length=1) <- c("A", "B", "C")
R> print(y3)                            # y3 has been changed
\end{Sinput}
\begin{Soutput}
[1] "A B C"
\end{Soutput}
\end{Schunk}



% \clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Code-pointwise comparing of strings}\label{Sec:fixed}

There are many situations where we are faced with testing whether two strings
(or parts thereof)
consist exactly of the same Unicode\footnote{All functions in \pkg{stringi}
automatically convert all \proglang{R} strings
to Unicode, see Section~\ref{Sec:encoding} for discussion.} code points, in the same order.
These include, for instance, matching a nucleotide sequence
in a DNA profile and querying for system resources based on file names or UUIDs.
Such tasks, due to their simplicity, can be performed very efficiently.


\subsection{Testing for equality of strings}


% equivalence relation on the set of characters

To quickly test whether the corresponding strings in two character vectors
are identical (in a code-pointwise manner), we can use the \code{\%s===\%}
operator or the \code{stri\_cmp\_eq()} function.

\begin{Schunk}
\begin{Sinput}
R> "actg" %s===% c("ACTG", "actg", "act", NA)  # recycling rule
\end{Sinput}
\begin{Soutput}
[1] FALSE  TRUE FALSE    NA
\end{Soutput}
\end{Schunk}

Moreover, \code{\%s!==\%} and \code{stri\_cmp\_neq()}, respectively,
are their negations.


\subsection{Searching for fixed strings}


\begin{table}[t!]
\centering
\begin{tabularx}{1.0\linewidth}{p{4.8cm}X}
\toprule
\bfseries Name(s) & \bfseries Meaning \\
\midrule
\code{stri\_count()} &  count pattern matches    \\
\midrule
\code{stri\_detect()} & detect pattern matches     \\
\midrule
\code{stri\_endswith()} &  [all but \code{regex}] detect pattern matches at end of string  \\
\midrule
\code{stri\_extract\_all()}, \code{stri\_extract\_first()}, \code{stri\_extract\_last()}  & extract pattern matches     \\
\midrule
\code{stri\_locate\_all()}, \code{stri\_locate\_first()}, \code{stri\_locate\_last()}  & locate pattern matches     \\
\midrule
\code{stri\_match\_all()}, \code{stri\_match\_first()}, \code{stri\_match\_last()}   &  [\code{regex} only] extract matches to regex capture groups   \\
\midrule
\code{stri\_replace\_all()}, \code{stri\_replace\_first()}, \code{stri\_replace\_last()}  &     substitute pattern matches with a replacement string \\
\midrule
\code{stri\_split()}  & split up a string at pattern matches     \\
\midrule
\code{stri\_startswith()}  &  [all but \code{regex}] detect pattern matches at start of string   \\
\midrule
\code{stri\_subset()}, \code{`stri\_subset<-`()}  & return or replace strings
that contain pattern matches \\
\bottomrule
\end{tabularx}

\caption{\label{Tab:searchfuns} String search/pattern matching functions in \pkg{stringi}.
Each function, unless otherwise indicated, can be used in conjunction
with any search engine, e.g., we have
\code{stri\_count\_fixed()} (see Section~\ref{Sec:fixed}),
\code{stri\_detect\_regex()} (see Section~\ref{Sec:regex}), and
\code{stri\_split\_coll()} (see Section~\ref{Sec:collator}).}
\end{table}



Table~\ref{Tab:searchfuns} lists the string search functions available
in \pkg{stringi}. Below we explain their behaviour in the context of fixed
pattern matching. Notably, their description
is very detailed, because -- as we shall soon find out --
similar search functions are available for the other string matching
engines (namely, those relying on  regular expressions
and the \pkg{ICU} Collator,
see Section~\ref{Sec:regex} and Section~\ref{Sec:collator}).
% It is also worth emphasising that, initially, the most basic string search API
% has been designed so as to be compatible with the early versions
% of the \pkg{stringr} package (compare Table~\ref{Tab:oldstringr}).
% In the course of \pkg{stringi}'s development, many new operations and extensions
% have been introduced.


For detecting if a string contains
a given substring  (code-pointwisely), the fast KMP \citep{KnuthETAL1977:kmp}
search algorithm, with worst time complexity of $O(n+p)$
(where $n$ is the length of the string and $p$ is the length of the pattern),
has been implemented in \pkg{stringi} (with numerous tweaks
for even faster matching).



\paragraph{Counting matches.}
The \code{stri_count_fixed()} function counts the number of
times a fixed pattern occurs in a given string.

\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed("abcabcdefabcabcabdc", "abc")  # search pattern is "abc"
\end{Sinput}
\begin{Soutput}
[1] 4
\end{Soutput}
\end{Schunk}

Equivalently, we can call the more generic (see below)
function \code{stri_count()} with  the \code{fixed=}\textit{pattern} argument:

\begin{Schunk}
\begin{Sinput}
R> stri_count("abcabcdefabcabcabdc", fixed="abc")
\end{Sinput}
\begin{Soutput}
[1] 4
\end{Soutput}
\end{Schunk}



\paragraph{Vectorisation.}
All the string search functions are vectorised
with respect to both the  \textit{haystack} and the \textit{needle} arguments
(and, e.g., the \textit{replacement} string, if applicable).
As usual, the shorter vector is recycled if necessary.
The users, unaware of this rule, might find this behaviour
unintuitive at the beginning, especially if
something does not go the way they expect. Therefore, let us point out
the most useful scenarios that are possible thanks to the arguments' recycling:

\begin{itemize}
\item many strings -- one pattern:

\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed(c("abc", "abcd", "abcabc", "abdc", "dab", NA), "abc")
\end{Sinput}
\begin{Soutput}
[1]  1  1  2  0  0 NA
\end{Soutput}
\end{Schunk}

\item one string -- many patterns:

\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed("abc", c("def", "bc", "abc", "abcde", NA))
\end{Sinput}
\begin{Soutput}
[1]  0  1  1  0 NA
\end{Soutput}
\end{Schunk}

\item each string -- its own corresponding pattern:

\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed(c("abc", "def", "ghi"), c("a", "z", "h"))
\end{Sinput}
\begin{Soutput}
[1] 1 0 1
\end{Soutput}
\end{Schunk}

\item each row in a matrix -- its own corresponding pattern:

\begin{Schunk}
\begin{Sinput}
R> (A <- matrix(
+    do.call(stri_paste,
+      expand.grid(
+        c("a", "b", "c"), c("a", "b", "c"), c("a", "b", "c")
+      )),
+    nrow=3))
\end{Sinput}
\begin{Soutput}
     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]
[1,] "aaa" "aba" "aca" "aab" "abb" "acb" "aac" "abc" "acc"
[2,] "baa" "bba" "bca" "bab" "bbb" "bcb" "bac" "bbc" "bcc"
[3,] "caa" "cba" "cca" "cab" "cbb" "ccb" "cac" "cbc" "ccc"
\end{Soutput}
\begin{Sinput}
R> matrix(stri_count_fixed(A, c("a", "b", "c")), nrow=3)
\end{Sinput}
\begin{Soutput}
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]    3    2    2    2    1    1    2    1    1
[2,]    1    2    1    2    3    2    1    2    1
[3,]    1    1    2    1    1    2    2    2    3
\end{Soutput}
\end{Schunk}

The above is due to the fact that matrices are
represented as ``flat'' vectors of length length \code{nrow(A)*ncol(A)},
whose elements are read in a column-major (\proglang{Fortran}) order.
Therefore, in the above example,
pattern \code{"a"} is being sought in the 1st, 4th, 7th,
\dots{} string in \code{A}, i.e., \code{"aaa"}, \code{"aba"}, \code{"aca"}, \dots;
pattern \code{"b"} in the 2nd, 5th, 8th, \dots{} string;
and \code{"c"} in the 3rd, 6th, 9th, \dots{} one.

\medskip
On a side note, to match different patterns
with respect to each \textit{column}, we can (amongst others)
apply matrix transpose twice (\code{t(stri_count_fixed(haystack, t(needle)))})
or use
the \code{rep()} function
to properly replicate the \textit{needles}:

\begin{Schunk}
\begin{Sinput}
R> (At <- t(A))  # example haystack
\end{Sinput}
\begin{Soutput}
      [,1]  [,2]  [,3]
 [1,] "aaa" "baa" "caa"
 [2,] "aba" "bba" "cba"
 [3,] "aca" "bca" "cca"
 [4,] "aab" "bab" "cab"
 [5,] "abb" "bbb" "cbb"
 [6,] "acb" "bcb" "ccb"
 [7,] "aac" "bac" "cac"
 [8,] "abc" "bbc" "cbc"
 [9,] "acc" "bcc" "ccc"
\end{Soutput}
\begin{Sinput}
R> matrix(stri_count_fixed(At, rep(c("a", "b", "c"), each=nrow(At))), ncol=3)
\end{Sinput}
\begin{Soutput}
      [,1] [,2] [,3]
 [1,]    3    1    1
 [2,]    2    2    1
 [3,]    2    1    2
 [4,]    2    2    1
 [5,]    1    3    1
 [6,]    1    2    2
 [7,]    2    1    2
 [8,]    1    2    2
 [9,]    1    1    3
\end{Soutput}
\end{Schunk}

A similar search in the case of a data frame-type input
(any list of character vectors of identical lengths)
can be performed by means of a call to \code{mapply()}:



\begin{Schunk}
\begin{Sinput}
R> (At.df <- as.data.frame(At))
\end{Sinput}
\begin{Soutput}
   V1  V2  V3
1 aaa baa caa
2 aba bba cba
3 aca bca cca
4 aab bab cab
5 abb bbb cbb
6 acb bcb ccb
7 aac bac cac
8 abc bbc cbc
9 acc bcc ccc
\end{Soutput}
\begin{Sinput}
R> mapply(stri_count_fixed, At.df, c("a", "b", "c"))
\end{Sinput}
\begin{Soutput}
      V1 V2 V3
 [1,]  3  1  1
 [2,]  2  2  1
 [3,]  2  1  2
 [4,]  2  2  1
 [5,]  1  3  1
 [6,]  1  2  2
 [7,]  2  1  2
 [8,]  1  2  2
 [9,]  1  1  3
\end{Soutput}
\end{Schunk}

\item all strings -- all patterns:

\begin{Schunk}
\begin{Sinput}
R> x <- c("aaa", "bbb", "ccc", "abc", "cba", "aab", "bab", "acc")
R> y <- c("a", "b", "c")
R> structure(
+    outer(x, y, stri_count_fixed),
+    dimnames=list(x, y)  # add row and column names
+  )
\end{Sinput}
\begin{Soutput}
    a b c
aaa 3 0 0
bbb 0 3 0
ccc 0 0 3
abc 1 1 1
cba 1 1 1
aab 2 1 0
bab 1 2 0
acc 1 0 2
\end{Soutput}
\end{Schunk}

A similar result (without the post-processing of the return value,
which can be done through a call to \code{matrix()})
may be obtained by calling:

\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed(rep(x, each=length(y)), y)
\end{Sinput}
\begin{Soutput}
 [1] 3 0 0 0 3 0 0 0 3 1 1 1 1 1 1 2 1 0 1 2 0 1 0 2
\end{Soutput}
\end{Schunk}


\end{itemize}



\paragraph{Search engine options.}
The pattern matching engine may be tuned  by passing
further arguments to the search functions (via ``\code{...}'';
they are be redirected as-is to \code{stri_opts_fixed()}).
Table~\ref{Tab:fixed_opts} gives the list of available options.





First, we may switch on the simplistic\footnote{Which is not suitable
for real-world NLP tasks, as it assumes
that changing the case of a single code point always produces one and only
one item;
This way, \code{"groß"} does not compare equal to \code{"GROSS"},
see Section~\ref{Sec:collator} (and partially Section~\ref{Sec:regex}) for a workaround.}
case-insensitive matching.


\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed("ACTGACGacgggACg", "acg", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[1] 3
\end{Soutput}
\end{Schunk}

Second, we can indicate whether we are interested in detecting
overlapping pattern matches or whether searching should continue
at the end of each match
(the latter being the default behaviour):

\begin{Schunk}
\begin{Sinput}
R> stri_count_fixed("acatgacaca", "aca")  # overlap=FALSE (default)
\end{Sinput}
\begin{Soutput}
[1] 2
\end{Soutput}
\begin{Sinput}
R> stri_count_fixed("acatgacaca", "aca", overlap=TRUE)
\end{Sinput}
\begin{Soutput}
[1] 3
\end{Soutput}
\end{Schunk}



\begin{table}[t!]
\centering

\begin{tabularx}{1.0\linewidth}{p{4cm}X}
\toprule
\bfseries{Option}            &\bfseries Purpose \\
\midrule
\code{case\_insensitive} & logical; whether to enable the simple
case-insensitive matching (defaults to \code{FALSE}) \\
\midrule
\code{overlap} & logical; whether to enable the detection of overlapping
matches (defaults to \code{FALSE});
available in \code{stri\_extract\_all\_fixed()},
\code{stri\_locate\_all\_fixed()}, and \code{stri\_count\_fixed()}
\\
\bottomrule
\end{tabularx}

\caption{\label{Tab:fixed_opts} Options for the fixed pattern search
engine, see \code{stri\_opts\_fixed()}.}
\end{table}



\paragraph{Detecting and subsetting patterns.}
A somehow simplified version of the above task involves asking  whether
a pattern occurs in a string at all. Such an operation can be performed
with a call to \code{stri\_detect\_fixed()}.

\begin{Schunk}
\begin{Sinput}
R> x <- c("abc", "abcd", "def", "xyzabc", "uabdc", "dab", NA, "abc")
R> stri_detect_fixed(x, "abc")
\end{Sinput}
\begin{Soutput}
[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE    NA  TRUE
\end{Soutput}
\end{Schunk}

We can also indicate that a no-match is rather of our  interest:

\begin{Schunk}
\begin{Sinput}
R> stri_detect_fixed(x, "abc", negate=TRUE)
\end{Sinput}
\begin{Soutput}
[1] FALSE FALSE  TRUE FALSE  TRUE  TRUE    NA FALSE
\end{Soutput}
\end{Schunk}

What is more, there is an option to stop searching
once a given number of matches has been found
in the \code{haystack} vector (as a whole):

\begin{Schunk}
\begin{Sinput}
R> stri_detect_fixed(x, "abc", max_count=3)
\end{Sinput}
\begin{Soutput}
[1]  TRUE  TRUE FALSE  TRUE    NA    NA    NA    NA
\end{Soutput}
\begin{Sinput}
R> stri_detect_fixed(x, "abc", negate=TRUE, max_count=2)
\end{Sinput}
\begin{Soutput}
[1] FALSE FALSE  TRUE FALSE  TRUE    NA    NA    NA
\end{Soutput}
\end{Schunk}

\noindent
This can be useful in scenarios such as ``find the first 5 matching
resource IDs''.


\medskip
There are also functions that verify whether a string
starts or ends\footnote{Note that testing for a pattern match at the start
or end of a string has not been implemented separately for \code{regex} patterns,
which support \code{"\textasciicircum"} and \code{"\$"} anchors that  serve exactly this very purpose.}
with a pattern match:

\begin{Schunk}
\begin{Sinput}
R> stri_startswith_fixed(x, "abc")  # from=1 - match at start
\end{Sinput}
\begin{Soutput}
[1]  TRUE  TRUE FALSE FALSE FALSE FALSE    NA  TRUE
\end{Soutput}
\begin{Sinput}
R> stri_endswith_fixed(x, "abc")    # to=-1 - match at end
\end{Sinput}
\begin{Soutput}
[1]  TRUE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE
\end{Soutput}
\end{Schunk}


\medskip
Pattern detection is often performed in conjunction
with character vector subsetting.
This is why we have a specialised (and hence slightly faster)
function that  returns only the strings that match a given pattern:




\begin{Schunk}
\begin{Sinput}
R> stri_subset_fixed(x, "abc")
\end{Sinput}
\begin{Soutput}
[1] "abc"    "abcd"   "xyzabc" NA       "abc"
\end{Soutput}
\end{Schunk}

The above is equivalent to \code{x[stri_detect_fixed(x, "abc")]}.
Moreover:

\begin{Schunk}
\begin{Sinput}
R> stri_subset_fixed(x, "abc", omit_na=TRUE)
\end{Sinput}
\begin{Soutput}
[1] "abc"    "abcd"   "xyzabc" "abc"
\end{Soutput}
\begin{Sinput}
R> stri_subset_fixed(x, "abc", negate=TRUE)  # all but the matches
\end{Sinput}
\begin{Soutput}
[1] "def"   "uabdc" "dab"   NA
\end{Soutput}
\end{Schunk}

There is also a replacement version of this function:

\begin{Schunk}
\begin{Sinput}
R> stri_subset_fixed(x, "abc") <- ""  # modifies x in-place
R> x
\end{Sinput}
\begin{Soutput}
[1] ""      ""      "def"   ""      "uabdc" "dab"   NA      ""
\end{Soutput}
\end{Schunk}





\paragraph{Locating and extracting patterns.}
The functions from the \code{stri_locate()} family
aim to pinpoint the positions of the matches to a pattern.
First, we may be interested in the location of the first or the
last pattern match:

\begin{Schunk}
\begin{Sinput}
R> x <- c("aga", "actg", NA, "ggAGAGAgaGAca", "agagagaga")
R> stri_locate_first_fixed(x, "aga")
\end{Sinput}
\begin{Soutput}
     start end
[1,]     1   3
[2,]    NA  NA
[3,]    NA  NA
[4,]    NA  NA
[5,]     1   3
\end{Soutput}
\begin{Sinput}
R> stri_locate_last_fixed(x, "aga")
\end{Sinput}
\begin{Soutput}
     start end
[1,]     1   3
[2,]    NA  NA
[3,]    NA  NA
[4,]    NA  NA
[5,]     7   9
\end{Soutput}
\end{Schunk}

\noindent
In both examples we obtain a two-column (``from--to'') matrix
with the number of rows determined by the recycling rule (here:
the length of \textit{x}).
Missing values correspond to either missing inputs or
no-matches.

Second, we may be yearning for the locations of all the matching
substrings. As the number of possible answers may differ from string to string,
the result is a list of ``from--to'' matrices.

\begin{Schunk}
\begin{Sinput}
R> stri_locate_all_fixed(x, "aga")
\end{Sinput}
\begin{Soutput}
[[1]]
     start end
[1,]     1   3

[[2]]
     start end
[1,]    NA  NA

[[3]]
     start end
[1,]    NA  NA

[[4]]
     start end
[1,]    NA  NA

[[5]]
     start end
[1,]     1   3
[2,]     5   7
\end{Soutput}
\end{Schunk}

\noindent
Note that, for compatibility with \pkg{stringr}, a no-match is
indicated by a single-row matrix with two missing values.
This behaviour can be changed by setting the \code{omit_no_match}
argument to \code{TRUE}. Here is an example that additionally
asks for overlapping, case insensitive matches:

\begin{Schunk}
\begin{Sinput}
R> stri_locate_all_fixed(x, "aga", omit_no_match=TRUE,
+    overlap=TRUE, case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[[1]]
     start end
[1,]     1   3

[[2]]
     start end

[[3]]
     start end
[1,]    NA  NA

[[4]]
     start end
[1,]     3   5
[2,]     5   7
[3,]     7   9
[4,]     9  11

[[5]]
     start end
[1,]     1   3
[2,]     3   5
[3,]     5   7
[4,]     7   9
\end{Soutput}
\end{Schunk}



\medskip
Let us recall that such kinds of ``from-to'' matrices constitute particularly
convenient inputs to \code{stri\_sub()} and \code{stri\_sub\_all()}.
However, if merely the extraction of the matching substrings is needed,
we can rely on the functions from the \code{stri\_extract()} family:

\begin{Schunk}
\begin{Sinput}
R> (res <- stri_extract_first_fixed(x, "aga", case_insensitive=TRUE))
\end{Sinput}
\begin{Soutput}
[1] "aga" NA    NA    "AGA" "aga"
\end{Soutput}
\begin{Sinput}
R> identical(res, stri_sub(x,
+    stri_locate_first_fixed(x, "aga", case_insensitive=TRUE)))
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
R> (res <- stri_extract_all_fixed(x, "aga",
+    overlap=TRUE, case_insensitive=TRUE, omit_no_match=TRUE))
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "aga"

[[2]]
character(0)

[[3]]
[1] NA

[[4]]
[1] "AGA" "AGA" "Aga" "aGA"

[[5]]
[1] "aga" "aga" "aga" "aga"
\end{Soutput}
\begin{Sinput}
R> identical(res, stri_sub_all(x,
+    stri_locate_all_fixed(x, "aga",
+      omit_no_match=TRUE, overlap=TRUE, case_insensitive=TRUE)))
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}





\paragraph{Replacing pattern occurrences.}
In order to replace each matching substring with a corresponding
replacement string, we can refer to \code{stri\_replace()}:

\begin{Schunk}
\begin{Sinput}
R> x <- c("aga", "actg", NA, "ggAGAGAgaGAca", "agagagaga")
R> stri_replace_first_fixed(x, "aga", "~", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[1] "~"           "actg"        NA            "gg~GAgaGAca" "~gagaga"
\end{Soutput}
\begin{Sinput}
R> stri_replace_last_fixed(x, "aga", "~", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[1] "~"           "actg"        NA            "ggAGAGAg~ca" "agagag~"
\end{Soutput}
\begin{Sinput}
R> stri_replace_all_fixed(x, "aga", "~", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[1] "~"         "actg"      NA          "gg~G~GAca" "~g~ga"
\end{Soutput}
\end{Schunk}

Note that the inputs that are not part of any match are left unchanged.

The function is vectorised with respect to all the three arguments
(\textit{haystack}, \textit{needle}, \textit{replacement string}),
with the usual recycling if necessary.
If a different arguments' vectorisation scheme is required,
we set the \code{vectorise_all} argument of \code{stri_replace_all()}
to \code{FALSE}.
Compare the following:

\begin{Schunk}
\begin{Sinput}
R> stri_replace_all_fixed("The quick brown fox jumped over the lazy dog.",
+    c("quick", "brown",      "fox", "lazy",    "dog"),
+    c("slow",  "yellow-ish", "hen", "spamity", "lama"))
\end{Sinput}
\begin{Soutput}
[1] "The slow brown fox jumped over the lazy dog."
[2] "The quick yellow-ish fox jumped over the lazy dog."
[3] "The quick brown hen jumped over the lazy dog."
[4] "The quick brown fox jumped over the spamity dog."
[5] "The quick brown fox jumped over the lazy lama."
\end{Soutput}
\begin{Sinput}
R> stri_replace_all_fixed("The quick brown fox jumped over the lazy dog.",
+    c("quick", "brown",      "fox", "lazy", "dog"),
+    c("slow",  "yellow-ish", "hen", "spamity", "lama"),
+    vectorise_all=FALSE)
\end{Sinput}
\begin{Soutput}
[1] "The slow yellow-ish hen jumped over the spamity lama."
\end{Soutput}
\end{Schunk}

\noindent
Here, for every string in the \textit{haystack}, we observe the vectorisation
\textit{independently} over the \textit{needles} and replacement strings.
Each occurrence of the 1st needle is substituted with the 1st replacement
string, then the search is repeated for the 2nd needle in order to replace
it with the 2nd corresponding string, and so forth.


\paragraph{Splitting.}
To  split each element in the \textit{haystack} into substrings,
where the \textit{needles} define the delimiters that separate
the inputs into tokens,
we call \code{stri\_split()}:

\begin{Schunk}
\begin{Sinput}
R> x <- c("a,b,c,d", "e", "", NA, "f,g,,,h,i,,j,")
R> stri_split_fixed(x, ",")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "a" "b" "c" "d"

[[2]]
[1] "e"

[[3]]
[1] ""

[[4]]
[1] NA

[[5]]
[1] "f" "g" ""  ""  "h" "i" ""  "j" ""
\end{Soutput}
\end{Schunk}

The result is a list of character vectors, as each string
in the \textit{haystack}
might be split into a possibly different number of tokens.

There are also options to omit empty strings from the resulting vectors,
or limit the number of tokens.


\begin{Schunk}
\begin{Sinput}
R> stri_split_fixed(x, ",", n=3)  # stringr compatibility mode
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "a"   "b"   "c,d"

[[2]]
[1] "e"

[[3]]
[1] ""

[[4]]
[1] NA

[[5]]
[1] "f"         "g"         ",,h,i,,j,"
\end{Soutput}
\begin{Sinput}
R> stri_split_fixed(x, ",", n=3, tokens_only=TRUE, omit_empty=TRUE)
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "a" "b" "c"

[[2]]
[1] "e"

[[3]]
character(0)

[[4]]
[1] NA

[[5]]
[1] "f" "g" "h"
\end{Soutput}
\end{Schunk}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section{Regular expressions}\label{Sec:regex}

Regular expressions (\textit{regex}es) provide us with a concise
grammar for defining systematic patterns which can be sought in character
strings, in particular:
\begin{enumerate}
  \item specific substrings,
  \item emojis of any kind,
  \item standalone sequences of lower-case Latin letters (``words''),
  \item substrings that can be interpreted as real numbers
  (with or without fractional part, also in scientific notation),
  \item telephone numbers,
  \item email addresses, or
  \item URLs.
\end{enumerate}
Theoretically, the concept of matching regular patterns
dates back to the so-called regular languages and finite state
automata \citep{kleene},
see also \citep{hopcroftullman,automata}.
Regexes in the form as we know today have already been present
in one of the pre-Unix implementations of the command-line text
editor \pkg{qed} (\citealp{qed}; the predecessor of the well-known \pkg{sed}).

Base  \proglang{R} gives access to two different regex matching engines
(via functions such as \code{gregexpr()} and \code{regexec()},
see Table~\ref{Tab:oldstringr}):
\begin{itemize}
\item {ERE}\footnote{Via the \pkg{TRE} library
(\url{https://github.com/laurikari/tre/}).}
(\textit{extended regular expressions} that conform
to the \pkg{POSIX.2-1992} standard);
used by default,
\item {PCRE}\footnote{Via the \pkg{PCRE2}
library (\url{https://www.pcre.org/}).}
(\textit{\proglang{Perl}-compatible regular expressions}),
in use if \code{perl = TRUE} is set.
\end{itemize}
Other matchers are implemented in the \pkg{ore}
(\citealp{ore}; via the \pkg{Onigmo} library)
and \pkg{re2r} (\citealp{re2r}; \pkg{RE2}) packages.
% https://cran.r-project.org/web/packages/ore/index.html
% https://github.com/qinwf/re2r


\pkg{Stringi}, on the other hand, provides access to the regex engine
implemented in \pkg{ICU}, which was inspired
by \proglang{Java}'s \pkg{util.regex}
in \pkg{JDK 1.4}. Their syntax is mostly compatible with that of \pkg{PCRE},
although certain advanced facets may not be supported (e.g., recursive
patters). On the other hand, \pkg{ICU} regexes fully conform to the
Unicode Technical Standard \#18 \citep{uts18:regex} and hence provide
comprehensive support for Unicode.


It is worth noting that most programming languages
as well as advanced text editors and IDEs (including \pkg{RStudio})
allow for finding or replacing patters with regexes.
Therefore, they should be amongst the instruments
at every data scientist's disposal.
One general introduction to regexes is \citep{friedl}.
% Some general topics are also covered in the R manual, see \code{?regex}.
The \pkg{ICU} flavour is summarised at
\url{http://userguide.icu-project.org/strings/regexp}.






Below we provide a concise yet comprehensive introduction
to the topic from the perspective of the \pkg{stringi} package users.
This time we will use the pattern search routines whose names
end with the \code{*_regex()} suffix.
Apart from \code{stri_detect_regex()}, \code{stri_locate_all_regex()},
and so forth, in Section~\ref{Sec:Capturing} we introduce
\code{stri_match_all_regex()},
\code{stri_match_first_regex()}, and
\code{stri_match_last_regex()}.
Moreover, Table~\ref{Tab:regex_opts} lists the available options
for the regex engine.


% string search functions described in Section~\ref{Sec:fixed},




\begin{table}[t!]
\centering

\begin{tabularx}{1.0\linewidth}{p{4.6cm}X}
\toprule
\bfseries{Option}            &\bfseries Purpose \\
\midrule
\code{case\_insensitive}\newline
[regex flag \code{(?i)}] & logical; defaults to \code{FALSE}; whether to enable (full) case-insensitive matching  \\
\midrule
\code{comments}\newline
[regex flag \code{(?x)}] & logical; defaults to \code{FALSE}; whether to allow white spaces and comments within patterns  \\
\midrule
\code{dot\_all}\newline
[regex flag \code{(?s)}] & logical; defaults to \code{FALSE}; if set, ``\code{.}'' matches line terminators, otherwise its matching stops at a line end  \\
\midrule
\code{literal} & logical; defaults to \code{FALSE}; whether to treat the entire pattern as a literal string; note that in most cases the code-pointwise string search facilities
(\code{*\_fixed()} functions described in Section~\ref{Sec:fixed}) are faster
\\
\midrule
\code{multi\_line}\newline
[regex flag \code{(?m)}] & logical; defaults to \code{FALSE}; if set, ``\code{\$}'' and ``\code{\textasciicircum}'' recognise line terminators within a string, otherwise, they match only at start and end of the input \\
\midrule
\code{unix\_lines} & logical; defaults to \code{FALSE}; when enabled, only the Unix line ending, i.e., \code{U+000a}, is honoured as a terminator by ``\code{.}'', ``\code{\$}'', and ``\code{\textasciicircum}''\\
\midrule
\code{uword}\newline
[regex flag \code{(?w)}] & logical; defaults to \code{FALSE}; whether to use the Unicode definition of word boundaries (see Section~\ref{Sec:BoundaryAnalysis}), which are quite different from the traditional regex word boundaries\\
\midrule
\code{error\_on\_unknown\_escapes} & logical; defaults to \code{FALSE}; whether unrecognised backslash-escaped characters trigger an error; by default,  unknown backslash-escaped ASCII letters represent themselves \\
\midrule
\code{time\_limit} & integer; processing time limit for match operations in $\sim$milliseconds
(depends on the CPU speed);
0 for no limit (the default) \\
\midrule
\code{stack\_limit} & integer; maximal size, in bytes, of the heap storage available
for the matcher's backtracking stack; setting a limit is desirable if poorly
written regexes are expected on input; 0 for no limit (the default) \\
\bottomrule
\end{tabularx}

\caption{\label{Tab:regex_opts} Options for the regular expressions
search engine, see \code{stri\_opts\_regex()}.}
\end{table}



\subsection{Matching individual characters}\label{Sec:RegexIndividualChars}

We shall begin by discussing different ways to define sets of characters.
In this part, the length of all matching substrings will be quite
easy to determine.

First, let's note that the following characters have special
meaning to the regex engine:
\[
\text{
  \texttt{.}\quad
  \textbackslash{}\quad
  \texttt{|}\quad
  \texttt{(}\quad
  \texttt{)}\quad
  \texttt{[}\quad
%    \texttt{]}
  \texttt{\{}\quad
  \texttt{\}}\quad
  \texttt{\^{}}\quad
  \texttt{\$}\quad
  \texttt{*}\quad
  \texttt{+}\quad
  \texttt{?}\quad
}
\]

Any regular expression that does not contain the above,
behaves like a fixed pattern:

\begin{Schunk}
\begin{Sinput}
R> x <- "spam, egg, spam, spam, bacon, and spam"
R> stri_count_regex(x, "spam")
\end{Sinput}
\begin{Soutput}
[1] 4
\end{Soutput}
\end{Schunk}

However, this time the case insensitive mode fully
supports Unicode matching\footnote{%
This does not mean, though, that it considers canonically
equivalent strings as equal,
see Section~\ref{Sec:Equivalence} for discussion and a workaround.}:
% matching strings may be of different lengths.
% stri_detect_regex("a\u0328", "ą")
% stri_detect_regex("ą", "a\u0328")

\begin{Schunk}
\begin{Sinput}
R> stri_detect_regex("groß", "GROSS", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}

If we wish to make a special character part of a regular expression --
so that it is treated literally -- we have to escape it with  a backslash,
``\textbackslash''. Yet, the backlash itself
has a special meaning to \proglang{R}, see \code{?Quotes},
therefore it needs to be preceded with another backslash.

\begin{Schunk}
\begin{Sinput}
R> stri_count_regex("spam...", "\\.")   # "\\" is a way to input a single \
\end{Sinput}
\begin{Soutput}
[1] 3
\end{Soutput}
\begin{Sinput}
R> stri_count_regex("spam...", r"(\.)") # literal string - since R 4.0
\end{Sinput}
\begin{Soutput}
[1] 3
\end{Soutput}
\end{Schunk}

In other words, the \proglang{R} string \code{"{}\textbackslash\textbackslash."{}}
is seen by the regex engine as ``\code{\textbackslash.}'' and interpreted
as the dot character (literally).


\paragraph{Matching any character.}
The (unescaped) dot, ``\code{.}'', matches any character except the newline.

\begin{Schunk}
\begin{Sinput}
R> x <- "Ham, spam, jam, SPAM, eggs, and spam"
R> stri_extract_all_regex(x, ".am")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "Ham" "pam" "jam" "pam"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, ".am", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "Ham" "pam" "jam" "PAM" "pam"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "..am", case_insensitive=TRUE)
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" " jam" "SPAM" "spam"
\end{Soutput}
\end{Schunk}




% \begin{ciekawostka}
The dot's insensitivity to the newline character is motivated
by the need to maintain the compatibility with tools such as \pkg{grep}
(when searching within text files in a line by line manner).
This behaviour can be altered by setting the \code{dot_all} option to \code{TRUE}:

\begin{Schunk}
\begin{Sinput}
R> x <- "ham, spam, jam\namalgam"
R> stri_extract_all_regex(x, ".am")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "ham" "pam" "jam" "gam"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, ".am", dot_all=TRUE)
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "ham"  "pam"  "jam"  "\nam" "gam"
\end{Soutput}
\end{Schunk}



\paragraph{Defining character sets.}
Sets of characters can be introduced by enumerating
their members between a pair of
square brackets.
For instance, ``\code{[abc]}'' denotes the set $\{\mathtt{a},\mathtt{b},\mathtt{c}\}$
-- such a regular expression matches one (and only one) symbol from this set.
Moreover, in:


\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x, "[hj]am")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "ham" "jam"
\end{Soutput}
\end{Schunk}


\noindent
the ``\str{[hj]am}'' regex matches:
``\code{h}'' or ``\code{j}'', followed by ``\code{a}'', followed by ``\code{m}''.
In other words, \code{"ham"} and \code{"jam"}
are the only two strings that are matched by this pattern
(unless matching is done case-insensitively).


% \begin{wazne}
The following characters, if used within square brackets, may be treated
non-literally:
\[
\text{
  \textbackslash{}\quad
  \texttt{[}\quad
  \texttt{]}\quad
  \texttt{\^{}}\quad
  \texttt{{}-{}}\quad
  \texttt{\&}\quad
}
\]
Therefore, to include them as-is in a character set, the
backslash-escape must be used.
For example, ``\code{[\textbackslash[\textbackslash]\textbackslash\textbackslash]}'' matches
the backslash or a square bracket.

\paragraph{Complementing sets.}
Including ``\code{\^{}}'' after the opening square bracket denotes the set complement.
Hence, ``\code{[\^{}abc]}'' matches any character except ``\code{a}'',
``\code{b}'', and ``\code{c}''.

\begin{Schunk}
\begin{Sinput}
R> x <- "Nobody expects the Spanish Inquisition!"
R> stri_extract_all_regex(x, "[^ ][^ ][^ ]")
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "Nob" "ody" "exp" "ect" "the" "Spa" "nis" "Inq" "uis" "iti" "on!"
\end{Soutput}
\end{Schunk}

\noindent
The above matches any substring that consists of 3 non-spaces.

\code{Defining Code Point Ranges.}
Each Unicode code point can be referenced by its unique numeric identifier,
see Section~\ref{Sec:codepoints}  for more details.
For instance, ``\code{a}'' is assigned code U+0061 and ``\code{z}'' is mapped to U+007A.
In the pre-Unicode era (mostly with regards to the ASCII codes, $\le$ U+007F,
representing English letters, decimal digits, some punctuation characters,
and a few control characters),
we were used to relying on specific code ranges; e.g.,
``\code{[a-z]}'' denotes the set comprised of all
characters with codes between U+0061 and U+007A, i.e., lowercase letters
of the English (Latin) alphabet.

\begin{Schunk}
\begin{Sinput}
R> x <- "In 2020, I had fun once."
R> stri_extract_all_regex(x, "[a-z]")        # codes U+0061 - U+007A
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "n" "h" "a" "d" "f" "u" "n" "o" "n" "c" "e"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "[0-9]")        # codes U+0030 - U+0039
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "2" "0" "2" "0"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "[A-Za-z0-9]")  # union of 3 code ranges
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "I" "n" "2" "0" "2" "0" "I" "h" "a" "d" "f" "u" "n" "o" "n" "c" "e"
\end{Soutput}
\end{Schunk}



\paragraph{Using predefined character sets.}
Each code point is assigned a unique \textit{general category},
which can be thought of a character's class,
see \citep{usa44:ucd}.
Sets of characters from each category can be referred to,
amongst others, by using the ``\code{\textbackslash{}p\{category\}}'' syntax:

% \begin{CJK*}{UTF8}{bsmi}
\begin{Schunk}
\begin{Sinput}
R> x <- "aąbßÆAĄB你123,.;'! \t-+=[]©←→”„²³¾"
R> stri_extract_all_regex(x, "\\p{L}")   # letter (equivalently: [\p{L}])
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "a"  "ą"  "b"  "ß"  "Æ"  "A"  "Ą"  "B"  "你"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\p{Ll}")  # lowercase letter
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "a" "ą" "b" "ß"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\p{Lu}")  # uppercase letter
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "Æ" "A" "Ą" "B"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\p{N}")   # number
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "1" "2" "3" "²" "³" "¾"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\p{P}")   # punctuation
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "," "." ";" "'" "!" "-" "[" "]" "”" "„"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\p{S}")   # symbol
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "+" "=" "©" "←" "→"
\end{Soutput}
\end{Schunk}
% \end{CJK*}


Characters' binary properties and scripts can also be referenced in a similar manner.
Some other predefined classes include:

% \begin{CJK*}{UTF8}{bsmi}
\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\w")     # word characters
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "a"  "ą"  "b"  "ß"  "Æ"  "A"  "Ą"  "B"  "你" "1"  "2"  "3"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\d")     # decimal digits, \p{Nd}
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "1" "2" "3"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\s")     # spaces, [\t\n\f\r\p{Z}]
\end{Sinput}
\begin{Soutput}
[[1]]
[1] " "  "\t"
\end{Soutput}
\end{Schunk}
% \end{CJK*}

Moreover, e.g., the upper-cased ``\code{\textbackslash{}P\{category\}}'' and
``\code{\textbackslash{}W}'' is equivalent to
``\code{[\^{}\textbackslash{}p\{category\}]}'' and
``\code{[\^{}\textbackslash{}w]}'', respectively, i.e.,
denote their complements.


% stri_extract_all_regex(z, "\\p{WHITE_SPACE}") # spaces (Unicode property)


\paragraph{Avoiding POSIX classes.}
The use of the POSIX-like character classes should be avoided.
The \pkg{ICU} User Guide states that in general they are not well-defined.

In particular, in POSIX-like regex engines, ``\code{[:punct:]}''
stands for the character class corresponding to the \code{ispunct()}
function in \proglang{C} (check out \code{man 3 ispunct} on Unix-like systems).
According to ISO/IEC 9899:1990 (ISO C90), \code{ispunct()} tests for
any printing character except for the space or a character for which \code{isalnum()}
is true.

In our case, \pkg{PCRE} yields:

\begin{Schunk}
\begin{Sinput}
R> x <- ",./|\\<>?;:'\"[]{}-=_+()*&^%$€#@!`~×‒„”"
R> stri_sub(x, gregexpr("[[:punct:]]",  x, perl=TRUE)[[1]], length=1)
\end{Sinput}
\begin{Soutput}
 [1] ","  "."  "/"  "|"  "\\" "<"  ">"  "?"  ";"  ":"  "'"  "\"" "["  "]"
[15] "{"  "}"  "-"  "="  "_"  "+"  "("  ")"  "*"  "&"  "^"  "%"  "$"  "#"
[29] "@"  "!"  "`"  "~"
\end{Soutput}
\begin{Sinput}
R> stri_sub(x, gregexpr("[^[:punct:]]", x, perl=TRUE)[[1]], length=1)
\end{Sinput}
\begin{Soutput}
[1] "€" "×" "‒" "„" "”"
\end{Soutput}
\end{Schunk}

However, in a POSIX setting, the details of the characters' belongingness
to particular classes depend on the current locale.
Therefore, ``\code{[:punct:]}'', in POSIX-like regex engines, is not portable.

\pkg{ICU}, on the other hand, gives:

\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x, "[[:punct:]]")    # equivalently: \p{P}
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] ","  "."  "/"  "\\" "?"  ";"  ":"  "'"  "\"" "["  "]"  "{"  "}"  "-"
[15] "_"  "("  ")"  "*"  "&"  "%"  "#"  "@"  "!"  "‒"  "„"  "”"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "[^[:punct:]]")   # complement
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "|" "<" ">" "=" "+" "^" "$" "€" "`" "~" "×"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\p{S}")         # symbols
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "|" "<" ">" "=" "+" "^" "$" "€" "`" "~" "×"
\end{Soutput}
\end{Schunk}

We strongly recommend, wherever possible, the use of the portable
``\code{[\textbackslash{}p\{P\}\textbackslash{}p\{S\}]}''
as an alternative to the \pkg{PCRE} ``\code{[:punct:]}''.



\subsection{Alternating and grouping subexpressions}

The alternation operator, ``\code{|}'',
allows us to match either its left or its right branch,
for instance:

\begin{Schunk}
\begin{Sinput}
R> x <- "spam, egg, ham, jam, algae, and an amalgam of spam, all al dente"
R> stri_extract_all_regex(x, "spam|ham")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" "ham"  "spam"
\end{Soutput}
\end{Schunk}

It has a very low precedence. Therefore, if we wish to
introduce an alternative of \textit{sub}expressions,
we need to group them, e.g., between round brackets\footnote{Which have
the side-effect of creating new capturing groups, see below for discussion.}:

\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x, "(sp|h)am")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" "ham"  "spam"
\end{Soutput}
\end{Schunk}

Matching is always done left-to-right, on a first-come, first-served basis.
Hence, if the left branch is a subset of the right one, the latter will
never be matched, as in the example below:

\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x, "(al|alga|algae)")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "al" "al" "al" "al"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "(algae|alga|al)")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "algae" "alga"  "al"    "al"
\end{Soutput}
\end{Schunk}

\paragraph{Non-grouping parentheses.}
Some parenthesised subexpressions where the opening bracket is followed by
the question mark have distinct meaning.
In particular, ``\str{(?\#...)}'' denote free-format comments
that are ignored by the regex parser:

\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x,
+    "(?# match 'sp' or 'h')(sp|h)(?# and 'am')am|(?# or match 'egg')egg")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" "egg"  "ham"  "spam"
\end{Soutput}
\end{Schunk}

\noindent
Nevertheless, constructing more sophisticated regexes by concatenating
subfragments thereof may sometimes be more readable:

\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex(x,
+    stri_paste(
+        "(sp|h)",   # match either 'sp' or 'h'
+        "am",       # followed by 'am'
+      "|",            # ... or ...
+        "egg"       # just match 'egg'
+  ))
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" "egg"  "ham"  "spam"
\end{Soutput}
\end{Schunk}


What is more, e.g., ``\str{(?i)}'' enables the \code{case\_insensitive}
mode.

\begin{Schunk}
\begin{Sinput}
R> stri_count_regex("Spam spam SPAMITY spAm", "(?i)spam")
\end{Sinput}
\begin{Soutput}
[1] 4
\end{Soutput}
\end{Schunk}

For more regex flags, we refer to Table~\ref{Tab:regex_opts}.




\subsection{Quantifiers}

Oftentimes, we need to enable the matching of a variable number of
instances of the same subexpression or make its presence totally optional.
This can be achieved with the following quantifiers:
\begin{itemize}
\item ``\str{?}'' matches 0 or 1 times,
\item ``\str{*}'' matches 0 or more times,
\item ``\str{+}'' matches 1 or more times,
\item ``\str{\{n,m\}}'' matches between \str{n} and \str{m} times,
\item ``\str{\{n,\}}'' matches at least \str{n} times,
\item ``\str{\{n\}}'' matches exactly \str{n} times.
\end{itemize}
These operators are applied to the preceding atoms.
For example, ``\str{ba+}'' is matched by
\strq{ba}, \strq{baa}, \strq{baaa}, \dots{} but not \strq{b} alone.


By default, the quantifiers are \textit{greedy} -- they match the
repeated subexpression as many times as possible. The ``\code{?}'' suffix (hence,
``\code{??}'', ``\code{*?}'', ``\code{+?}'', and so forth) tries with
as few occurrences as possible (to still get a match).


\begin{Schunk}
\begin{Sinput}
R> x <- "sp(AM)(maps)(SP)am"
R> stri_extract_all_regex(x,
+    c("\\(.+\\)",    # [[1]] greedy
+      "\\(.+?\\)",   # [[2]] lazy
+      "\\([^)]+\\)"  # [[3]] greedy (but clever)
+  ))
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "(AM)(maps)(SP)"

[[2]]
[1] "(AM)"   "(maps)" "(SP)"

[[3]]
[1] "(AM)"   "(maps)" "(SP)"
\end{Soutput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
R> stri_extract_first_regex("spamamamnomnomnomammmmmmmmm",
+    c("sp(am|nom)+",             "sp(am|nom)+?",
+      "sp(am|nom)+?m*",          "sp(am|nom)+?m+"))
\end{Sinput}
\begin{Soutput}
[1] "spamamamnomnomnomam"         "spam"
[3] "spam"                        "spamamamnomnomnomammmmmmmmm"
\end{Soutput}
\end{Schunk}



Let us stress that the quantifier is applied to the subexpression
that stands directly before it. Grouping parentheses can be used in case
they are needed.

\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex("12, 34.5, 678.901234, 37...629, ...",
+    c("\\d+\\.\\d+",
+      "\\d+\\.\\d+?",
+      "\\d+(\\.\\d+)?"))
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "34.5"       "678.901234"

[[2]]
[1] "34.5"  "678.9"

[[3]]
[1] "12"         "34.5"       "678.901234" "37"         "629"
\end{Soutput}
\end{Schunk}





% \begin{table}[t!]
% \caption{\label{Tab:RegExpQuantifiers} Regex quantifiers.}
% \centering\small
% % \scriptsize
% \begin{tabular}{llc}
% \toprule
% \small\bf Operator & \small\bf Meaning & \small\bf Notes  \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{?}
% & match 0 or 1 times
% &  yes
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{*}
% & match 0 or more times
% & yes
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{+}
% & match 1 or more times
% & yes
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{\{n,m\}}
% & match between \str{n} and \str{m} times
% & yes
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{\{n,\}}
% & match at least \str{n} times
% & yes
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{\{n\}}
% & match exactly \str{n} times
% & nd.
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{??}
% & powtórz 0 razy lub 1 raz
% &  no
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{*?}
% & powtórz 0 lub więcej razy
% & no
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{+?}
% & powtórz 1 raz lub więcej
% & no
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{\{n,m\}?}
% & powtórz od \str{n} do \str{m} razy
% & no
% \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \midrule
% \str{\{n,\}?}
% & powtórz co najmniej \str{n} razy
% & no
% \\
% \bottomrule
% \end{tabular}
% \end{table}


\paragraph{Performance notes.}
\pkg{ICU}, just like \pkg{PCRE}, uses a nondeterministic finite automaton-type
algorithm. Hence, due to backtracking, some ill-defined regexes can lead to
exponential matching times (e.g., ``\code{(a+)+b}'' applied on \strq{aaaa...aaaaac}).
If such patterns are expected, setting the \code{time\_limit} or \code{stack\_limit}
option is recommended.

\begin{Schunk}
\begin{Sinput}
R> system.time(tryCatch({
+    stri_detect_regex("a" %s*% 1000 %s+% "c", "(a+)+b", time_limit=1e5)
+  }, error=function(e) cat("stopped.")))
\end{Sinput}
\begin{Soutput}
stopped.
\end{Soutput}
\begin{Soutput}
   user  system elapsed
 20.664   0.000  20.664
\end{Soutput}
\end{Schunk}


Nevertheless, oftentimes such regexes can be naturally
reformulated to  fix the underlying issue.
The \pkg{ICU} User Guide on Regular Expressions also recommends using
\textit{possessive quantifiers} (``\code{?+}'', ``\code{*+}'', ``\code{++}'', and so on),
which match as many times as possible but, contrary to
the plain-greedy ones, never backtrack when they happen to consume too much data.

See also the \pkg{re2r} package (a wrapper around the \pkg{RE2} library;
\citealp{re2r})
documentation and the references therein for discussion.




\vbox{
\subsection{Capture groups and references thereto}\label{Sec:Capturing}

It turns out that round-bracketed subexpressions carries one additional
characteristic: they form the so-called \textit{capture groups} that can
be extracted separately or be referred to in other parts of the same regex.
}



\paragraph{Extracting capture group matches.}
This is most evident when we use the
capture group-sensitive versions of \code{stri\_extract()}:
\code{stri\_match\_first\_regex()},
\code{stri\_\-match\_\-last\_\-regex()}, and
\code{stri\_match\_all\_regex()}.


\begin{Schunk}
\begin{Sinput}
R> x <- "name='Sir Launcelot', quest='Seek the Grail', colour='blue'"
R> stri_extract_all_regex(x, "(\\w+)='(.+?)'")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "name='Sir Launcelot'"   "quest='Seek the Grail'"
[3] "colour='blue'"
\end{Soutput}
\begin{Sinput}
R> stri_match_all_regex(x, "(\\w+)='(.+?)'")
\end{Sinput}
\begin{Soutput}
[[1]]
     [,1]                     [,2]     [,3]
[1,] "name='Sir Launcelot'"   "name"   "Sir Launcelot"
[2,] "quest='Seek the Grail'" "quest"  "Seek the Grail"
[3,] "colour='blue'"          "colour" "blue"
\end{Soutput}
\end{Schunk}

In the latter example, we follow the convention introduced in \pkg{stringr},
where the findings are presented in a matrix form. The first column
gives the complete matches, the second column stores the matches to the
first capture group, and so forth.

If we just need the grouping part of ``\code(...)'', i.e.,
without the capturing feature,
``\code{(?:\dots)}'' can be applied:

\begin{Schunk}
\begin{Sinput}
R> stri_match_all_regex(x, "(?:\\w+)='(.+?)'")
\end{Sinput}
\begin{Soutput}
[[1]]
     [,1]                     [,2]
[1,] "name='Sir Launcelot'"   "Sir Launcelot"
[2,] "quest='Seek the Grail'" "Seek the Grail"
[3,] "colour='blue'"          "blue"
\end{Soutput}
\end{Schunk}


\paragraph{Replacing with capture group matches.}
Matches to particular capture groups can be recalled in replacement strings
when using \code{stri\_replace()}.
Here, the match in its entirety is denoted with ``\str{\$0}'',
``\str{\$1}'' stores whatever was caught by the first capture group,
``\str{\$2}'' is the match to the second capture group, etc.
Moreover, ``\str{\textbackslash{}\$}'' gives the dollar-sign.


\begin{Schunk}
\begin{Sinput}
R> stri_replace_all_regex(x, "(\\w+)='(.+?)'", "$2 is a $1")
\end{Sinput}
\begin{Soutput}
[1] "Sir Launcelot is a name, Seek the Grail is a quest, blue is a colour"
\end{Soutput}
\end{Schunk}

\paragraph{Back-referencing.}
Matches to capture groups can also be part of the regexes themselves.
For example, ``\code{\textbackslash{}1}'' denotes whatever
has been consumed by the first capture group.

Although, in general, parsing of \proglang{HTML} code with regexes is
not recommended, let us consider the following examples:

\begin{Schunk}
\begin{Sinput}
R> x <- "<strong><em>spam</em></strong><code>eggs</code>"
R> stri_extract_all_regex(x, "<[a-z]+>.*?</[a-z]+>")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "<strong><em>spam</em>" "<code>eggs</code>"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "<([a-z]+)>.*?</\\1>")  # \1 - back-reference
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "<strong><em>spam</em></strong>" "<code>eggs</code>"
\end{Soutput}
\end{Schunk}

The second regex guarantees that the match will  include all characters
between the opening \code{<tag>} and the \textit{corresponding} (not: any)
closing \code{</tag>}.




\bigskip
On a side note, currently \pkg{ICU} does not support the extraction
of names of named capture groups, see however \citep{namedCapture}
for discussion.



\subsection{Anchoring}

Lastly, let us discuss ways to match a pattern
at a given abstract position within a string.

\paragraph{Matching at the beginning or end of a string.}
``\str{\^{}}'' and ``\str{\$}'' allow us to match, respectively,
start and end of the string
(or each line within a string, if the \code{multi_line} option is set to \code{TRUE}).

\begin{Schunk}
\begin{Sinput}
R> x <- c("spam egg", "bacon spam", "spam", "egg spam bacon")
R> stri_detect_regex(x, "spam")           # 'spam' wherever
\end{Sinput}
\begin{Soutput}
[1] TRUE TRUE TRUE TRUE
\end{Soutput}
\begin{Sinput}
R> stri_detect_regex(x, "^spam")          # begins with 'spam'
\end{Sinput}
\begin{Soutput}
[1]  TRUE FALSE  TRUE FALSE
\end{Soutput}
\begin{Sinput}
R> stri_detect_regex(x, "spam$")          # ends with 'spam'
\end{Sinput}
\begin{Soutput}
[1] FALSE  TRUE  TRUE FALSE
\end{Soutput}
\begin{Sinput}
R> stri_detect_regex(x, "^spam$")         # 'spam' only
\end{Sinput}
\begin{Soutput}
[1] FALSE FALSE  TRUE FALSE
\end{Soutput}
\begin{Sinput}
R> stri_detect_regex(x, "spam$|^spam")    # begins or ends with 'spam'
\end{Sinput}
\begin{Soutput}
[1]  TRUE  TRUE  TRUE FALSE
\end{Soutput}
\end{Schunk}


\paragraph{Matching at word boundaries.}
Furthermore, ``\str{\textbackslash{}b}'' matches
at a ``word boundary``, e.g., near spaces, punctuation marks,
or at the start/end of a string (i.e., wherever there is a transition
between a word, ``{\str{\textbackslash{}w}}'', and a non-word character,
``{\str{\textbackslash{}W}}'' or vice versa).

In the two following examples we match
all complete ``words'' that end with \strq{am} (not just any string
that includes \strq{am})
and all stand-alone numbers\footnote{This regex is for didactic purposes only.}:


\begin{Schunk}
\begin{Sinput}
R> stri_extract_all_regex("spam, spams, jam, tramway", "\\b\\w*am\\b")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" "jam"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex("12, 34.5, J23, 37.629cm", "\\b\\d+(\\.\\d+)?+\\b")
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "12"   "34.5"
\end{Soutput}
\end{Schunk}



\paragraph{Looking behind and ahead.}
There are also ways to guarantee that a pattern occurrence
begins or ends with a match to some subexpression:
``\str{(?<=...)...}'' is the so-called  \textit{look-behind}, whereas
``\str{...(?=...)}'' denotes the \textit{look-ahead}.
Moreover, ``\str{(?<!...)...}'' and ``\str{...(?!...)}'' are
their negated (``negative look behind/ahead'') versions.



\begin{Schunk}
\begin{Sinput}
R> x <- "I like spam, spam, eggs, and spam."
R> stri_extract_all_regex(x, "\\w+(?=[,.])") # word that ends with ',' or '.'
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "spam" "spam" "eggs" "spam"
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_regex(x, "\\w++(?![,.])") # neither ends with ',' nor '.'
\end{Sinput}
\begin{Soutput}
[[1]]
[1] "I"    "like" "and"
\end{Soutput}
\end{Schunk}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{String collation}\label{Sec:collator}



Historically, code-pointwise comparison had been used in most string comparison
activities, especially if strings in ASCII (i.e., English) were
involved. However, nowadays %(in the ``Internet era'')
this does not necessarily constitute the most suitable
approach to the processing of natural-language texts.
In particular, a code point vs.~code point matching
does \textit{not} take into account
accented and conjoined letters as well as ignorable punctuation and case.
% see Section~\ref{Sec:collator} for more details and an NLP-aware solution.



The \pkg{ICU} Collation Service\footnote{See the \pkg{ICU} User Guide on {Collation},
\url{http://userguide.icu-project.org/collation}.}
provides the basis for such string comparison activities as
string sorting and searching, or determining if two strings are equivalent.
This time, though, due to its conformance to
the Unicode Collation Algorithm \citep{uts10:collation},
% (see also ISO/IEC 14651:2016),
we may expect that the generated results
will meet the requirements of the culturally correct
natural language processing in any \textit{locale}.





\subsection{Locales}

String collation is amongst many locale-sensitive operations  available
in \pkg{stringi}. Before proceeding any further, we should
first discuss how we can parameterise the \pkg{ICU} services
so as to deliver the results that reflect the expectations
of a specific user community, such as the speakers of different languages
and their various regional variants.

\paragraph{Specifying locales.}
A locale specifier\footnote{%
Locale specifiers in \pkg{ICU} are platform-independent.
This is not the case for their base-\proglang{R} counterparts, see
\code{?locales}, e.g., we have \strq{Polish\_Poland} on Windows
vs.~\strq{pl\_PL} on Linux.}
is of the form
\code{"Language"}, \code{"Language_Country"}, or \code{"Language_Country_Variant"},
where:
\begin{itemize}
\item
\code{Language} is, most frequently, a two- or three-letter code that conforms to
the ISO-639-1 or ISO-630-2 standard, respectively;
e.g., \code{"en"} or \code{"eng"} for English, \code{"es"} or \code{"spa"}
for Spanish, \code{"zh"} or \code{"zho"} for Chinese, and \code{"mas"} for Masai
(which lacks the corresponding two-letter code);
however, more specific language identifiers may also be
available, e.g., \code{"zh_Hans"} for Simplified-
and \code{"zh_Hant"} for Traditional-Chinese
or \code{"sr_Cyrl"} for Cyrillic- and \code{"sr_Latn"} Latin-Serbian;


\item
\code{Country} is a two-letter code following the ISO-3166 standard
that enables different language conventions within the same language;
e.g., the US-English (\code{"en_US"}) and Australian-English (\code{"en_AU"})
not only observe some differences in spelling and vocabulary, but also
in the units of measurement;

\item
\code{Variant} is an identifier indicating a preference towards
some convention within the same country; e.g., \code{"de_DE_PREEURO"}
formats currency values using the pre-2002 Deutsche Mark (DEM).
\end{itemize}
Moreover,  following the
``\code{@}'' symbol, semicolon-separated ``\code{key=value}'' pairs
can be appended to the locale specifier, in order to
customise some locale-sensitive services even further
(see below for an example using ``\code{@collation=phonebook}''
and Section~\ref{Sec:datetime} for ``\code{@calendar=hebrew}'', amongst others).


{\color{red}

}
%
% (GNU/Linux)	pl_PL.iso-8859-2
% (GNU/Linux)	Polish_Poland.1250
%





\paragraph{Listing locales.}
To list the available locale identifiers, we call \code{stri_locale_list()}.

\begin{Schunk}
\begin{Sinput}
R> length(stri_locale_list())     # number of available locales
\end{Sinput}
\begin{Soutput}
[1] 722
\end{Soutput}
\begin{Sinput}
R> sample(stri_locale_list(), 5)  # 5 random ones
\end{Sinput}
\begin{Soutput}
[1] "pt_PT"  "seh_MZ" "fr_GP"  "en_RW"  "en_IO"
\end{Soutput}
\end{Schunk}



\paragraph{Querying for locale-specific services.}
The availability of locale-specific services can only be determined during
the very request for a particular resource\footnote{For more details,
see the \pkg{ICU} User Guide on {Locales}, \url{http://userguide.icu-project.org/locale}.}.
It may depend on the \pkg{ICU} library version
actually in use as well as the way the \pkg{ICU} Data Library (\pkg{icudt})
has been packaged. Therefore, for maximum portability,
it is best to rely on the \pkg{ICU} library bundle that
is shipped with \pkg{stringi}.
This is the case on Windows and OS~X, whose users typically download the
pre-compiled versions of the package from CRAN.
However, on various flavours of GNU/Linux and other Unix-based systems,
the system \pkg{ICU} is used more eagerly\footnote{
See, e.g., software packages
\code{libicu-dev} on Debian/Ubuntu or \code{libicu-devel} on RHL/Fedora/OpenSUSE.
For more details regarding the configure/build process of \pkg{stringi},
refer to the \code{INSTALL} file.}.
To force building \pkg{ICU} from sources, we may call:

\begin{Schunk}
\begin{Sinput}
R> install.packages("stringi", configure.args="--disable-pkg-config")
\end{Sinput}
\end{Schunk}

Overall, should a requested service be unavailable
in a given locale, the best possible match is returned.


\paragraph{Default locale.}
Each locale-sensitive operation in \pkg{stringi} selects the \textit{current
default locale} if no locale has been explicitly requested,
i.e., when a function's \code{locale} argument (see Table~\ref{Tab:collator_opts})
is left alone in its ``\code{NULL}'' state.
The default locale is initially set to match the system locale on the current
platform, and may be changed with \code{stri_locale_set()}, e.g.,
in the very rare case of improper automatic locale detection.



\begin{Schunk}
\begin{Sinput}
R> stri_locale_get()
\end{Sinput}
\begin{Soutput}
[1] "en_AU"
\end{Soutput}
\end{Schunk}


% locales in base R -- very problematic
%
% Sys.getlocale
%
% Sys.setlocale
%
% no per-call basis
%
% might be not installed
%
% different identifiers on Windows







\subsection{Testing string equivalence}\label{Sec:Equivalence}



% Canonical and Compatibility Equivalence -> http://unicode.org/reports/tr15/
In Unicode, some characters may have multiple representations.
For instance, ``LATIN SMALL LETTER A WITH OGONEK'' (``ą'') can be stored
as a single code point U+0105 or as a sequence
that is comprised of the letter ``LATIN SMALL LETTER A'', U+0061, and
the ``{COMBINING OGONEK}'', U+0328 (when rendered properly, they appear
as if they were identical glyphs).
This is an example of \textit{canonical equivalence} of strings.

Testing for the Unicode equivalence between strings
can be performed by calling \code{\%s==\%} and, more generally,
\code{stri\_cmp\_equiv()}, or their negated versions,
\code{\%s!=\%} and \code{stri\_cmp\_nequiv()}.

\begin{Schunk}
\begin{Sinput}
R> "a\u0328" %s==% "ą"             # a, ogonek == a with ogonek
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
R> stri_cmp_equiv("a\u0328", "ą")  # the same
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}



There are also functions for indicating and removing
duplicated elements in a character vector:

\begin{Schunk}
\begin{Sinput}
R> x <- c("Gągolewski", "Gagolewski", "Ga\u0328golewski")
R> stri_unique(x)
\end{Sinput}
\begin{Soutput}
[1] "Gągolewski" "Gagolewski"
\end{Soutput}
\begin{Sinput}
R> stri_duplicated(x)
\end{Sinput}
\begin{Soutput}
[1] FALSE FALSE  TRUE
\end{Soutput}
\begin{Sinput}
R> stri_duplicated(x, from_last=TRUE)
\end{Sinput}
\begin{Soutput}
[1]  TRUE FALSE FALSE
\end{Soutput}
\begin{Sinput}
R> stri_duplicated_any(x)  # index of the first non-unique element
\end{Sinput}
\begin{Soutput}
[1] 3
\end{Soutput}
\end{Schunk}







\subsection{Linear ordering of strings}


% https://www.elastic.co/guide/en/elasticsearch/guide/current/sorting-collations.html

Operators such that \code{\%s<\%}, \code{\%<=\%}, etc.,
and the corresponding functions
\code{stri_cmp_lt()} (``less than''),
\code{stri_cmp_le()} (``less than or equal''), etc.,
implement locale-sensitive linear orderings of strings.
Moreover, \code{stri_sort()} returns the lexicographically-sorted
version of a given input vector and \code{stri_order()} yields
the corresponding
(stable) ordering permutation.


% contextual e.g., Slovak ch h c

\begin{Schunk}
\begin{Sinput}
R> "chaotic" %s<% "hard" # current default locale (here: en_AU)
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
R> stri_cmp_lt("chłodny", "hardy", locale="pl_PL")  # Polish
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
R> stri_cmp_lt("chladný", "hladný", locale="sk_SK") # Slovak
\end{Sinput}
\begin{Soutput}
[1] FALSE
\end{Soutput}
\begin{Sinput}
R> stri_cmp("chladný", "hladný", locale="sk_SK")    # -1,0,1 encode <,=,>
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}

Note that the locale-aware comparison might be context-sensitive
and goes beyond the simple code-pointwise comparison.
In the example above, a \textit{contraction} occurred: in the Slovak language,
two code points ``\code{ch}'' are treated as a single entity
and are sorted after ``\code{h}'':

\begin{Schunk}
\begin{Sinput}
R> stri_sort(c("chłodny", "hardy", "cichy", "cenny"), locale="pl_PL")
\end{Sinput}
\begin{Soutput}
[1] "cenny"   "chłodny" "cichy"   "hardy"
\end{Soutput}
\begin{Sinput}
R> stri_sort(c("cudný", "chladný", "hladný", "čudný"), locale="sk_SK")
\end{Sinput}
\begin{Soutput}
[1] "cudný"   "čudný"   "hladný"  "chladný"
\end{Soutput}
\end{Schunk}

% stri_sort(c("cudný", "chladný", "hladný"), locale="cs_CZ")

An opposite situation is called an \textit{expansion}:
\begin{Schunk}
\begin{Sinput}
R> german_k_words <- c("können", "kondensieren", "kochen", "korrelieren")
R> stri_sort(german_k_words, locale="de_DE")
\end{Sinput}
\begin{Soutput}
[1] "kochen"       "kondensieren" "können"       "korrelieren"
\end{Soutput}
\begin{Sinput}
R> stri_sort(german_k_words, locale="de_DE@collation=phonebook")
\end{Sinput}
\begin{Soutput}
[1] "kochen"       "können"       "kondensieren" "korrelieren"
\end{Soutput}
\end{Schunk}

In the latter example, where we use the German phone-book order,
\code{"{}ö"{}} is treated as \code{"{}oe"{}}.



%
% Z~punktu~\ref{Subsec:relacyjne} wiemy, że na napisach
% możemy stosować dostępne w~pakiecie bazowym operatory relacyjne,
% np.~``\code{==}'' czy ``\code{<=}''. Niestety, operatory te
% działają w~oparciu o~informacje uzyskiwane od systemu operacyjnego
% i~tym samym mogą dawać różne wyniki na różnych platformach.
%
% W~pakiecie \pkg{stringi} porządek liter i~innych
% reguł właściwych dla  stosowanego domyślnie na naszym komputerze języka naturalnego
% określa zestaw ustawień regionalnych
% (ang.~\textit{locale}), których identyfikator możemy poznać, wywołując:
%
%
% <<>>=
% stri_locale_get()
% @
%
% Odpowiednikami bazowych operatorów relacyjnych
% są więc tu m.in. ``\code{\%s<\%}'' oraz ``\code{\%s==\%}''
% a~także funkcje
% \code{stri\_cmp\_lt()},
% \code{stri\_cmp\_eq()} itd.



\subsection{Collator options}\label{Sec:collator_opts}

Table~\ref{Tab:collator_opts} lists the options
that can be passed to \code{stri_opts_coll()} via ``\code{...}'' in
all the functions that rely on the \pkg{ICU} Collator.
Below we would like to attract the reader's attention
to some of them.

% http://userguide.icu-project.org/collation - many nice examples

\begin{table}[t!]
\centering

\begin{tabularx}{1.0\linewidth}{lX}
\toprule
\bfseries{Option}            &\bfseries Purpose \\
\midrule
\code{locale}             & a string specifying the locale to use; \code{NULL}
(default) or \code{"{}"{}} for the current default locale as indicated by
\code{stri\_locale\_get()} \\
\midrule
\code{strength}           & an integer in $\{1,2,3,4\}$ defining collation strength;
1 for the most permissive collation rules, 4 for the strictest ones;
defaults to 3 \\
\midrule
\code{uppercase\_first}    & logical; \code{NA} (default) orders upper
and lower
case letters in accordance to their tertiary weights, \code{TRUE} forces upper
case letters to sort before lower case letters, \code{FALSE} does the opposite \\
\midrule
\code{numeric}            & logical; if \code{TRUE}, a collation key
for the numeric value of substrings of digits is generated; this is a way to
make \code{"100"} ordered
after \code{"2"}; defaults to \code{FALSE} \\
\midrule
\code{case\_level}         & logical; if \code{TRUE}, an extra case level
(positioned before the third level) is generated; defaults to \code{FALSE} \\
\midrule
\code{normalisation}      & logical; if \code{TRUE}, then an incremental
check is performed to see whether the input data is in the FCD (``fast C or D'') form;
if the data is not in the FCD form, the incremental NFD normalisation is performed,
see Section~\ref{Sec:normalisation}; defaults to \code{FALSE}   \\
\midrule
\code{alternate\_shifted}  & logical; if \code{FALSE} (default),
all code points with non-ignorable primary weights are handled in the same way;
\code{TRUE} causes the code points
with primary weights that are equal or below the variable top value
to be ignored on the primary level and moved to the quaternary level; this can be
used to, e.g., ignore punctuation, see examples provided \\ \midrule
\code{french}             & logical; \code{TRUE} results in secondary
weights being considered backwards, i.e., ordering according to the last accent difference
-- nowadays only used in Canadian French; defaults to \code{FALSE} \\
\bottomrule
\end{tabularx}

\caption{\label{Tab:collator_opts} Options for the \pkg{ICU} Collator that can be passed to \code{stri\_opts\_collator()}.}
\end{table}



\paragraph{Collation strength.}
The Unicode Collation Algorithm \citep{uts10:collation}
can go beyond simple canonical equivalence
and allow us to treat some other (depending on the context)
differences as \textit{negligible}.

The \code{strength} option controls the Collator's ``attention to detail''.
For instance, it can be used to make the ligature ``ff'' (U+FB00)
compare equal to the two-letter sequence ``f{}f'':

\begin{Schunk}
\begin{Sinput}
R> stri_cmp_equiv("\ufb00", "ff")
\end{Sinput}
\begin{Soutput}
[1] FALSE
\end{Soutput}
\begin{Sinput}
R> stri_cmp_equiv("\ufb00", "ff", strength=2)
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}



Generally, four (nested) levels of inter-string differences can be distinguished:
% http://userguide.icu-project.org/collation/concepts
\begin{enumerate}
\item A primary difference -- the strongest one -- occurs where
there is a mismatch between base characters (e.g., \code{"a"} vs.~\code{"b"}).

\item Some character accents can be considered a secondary difference
in many languages. However, in other ones, an accented letter is considered
a different letter.

\item Distinguishing between upper- and lower case typically happens
on the tertiary level, see, however, the \texttt{case\_level} option.


\item If \code{alternate\_shifted} is \code{TRUE},
differences in punctuation
can be determined at the quaternary level. This is also meaningful
in the processing of Hiragana text.


\end{enumerate}


\paragraph{Ignoring case.}
Note what follows:

\begin{Schunk}
\begin{Sinput}
R> x <- c("gro\u00df", "gross", "GROSS", "Gro\u00df", "Gross")
R> stri_unique(x, strength=1)                  # ß == ss, case insensitive
\end{Sinput}
\begin{Soutput}
[1] "groß"
\end{Soutput}
\begin{Sinput}
R> stri_unique(x, strength=1, case_level=TRUE) # ß == ss, case sensitive
\end{Sinput}
\begin{Soutput}
[1] "groß"  "GROSS" "Groß"
\end{Soutput}
\begin{Sinput}
R> stri_unique(x, strength=2)                  # ß != ss, case insensitive
\end{Sinput}
\begin{Soutput}
[1] "groß"  "gross"
\end{Soutput}
\end{Schunk}

\paragraph{Ignoring some punctuation.}
Here are some effects of changing the  \code{alternate\_shifted} option:


\begin{Schunk}
\begin{Sinput}
R> x <- c("code point", "code-point", "codepoint", "CODE POINT", "CodePoint")
R> stri_unique(x, alternate_shifted=TRUE)  # strength=3
\end{Sinput}
\begin{Soutput}
[1] "code point" "CODE POINT" "CodePoint"
\end{Soutput}
\begin{Sinput}
R> stri_unique(x, alternate_shifted=TRUE, strength=2)
\end{Sinput}
\begin{Soutput}
[1] "code point"
\end{Soutput}
\begin{Sinput}
R> stri_unique(x, strength=2)
\end{Sinput}
\begin{Soutput}
[1] "code point" "code-point" "codepoint"
\end{Soutput}
\end{Schunk}



%
% % outer(x, x, stri_cmp, strength=2)
%
% primary secondary tertiary quaternary  equal
%
%  	A and B are equivalent (equal at all levels)
%
%  	canonically equivalent: the sequences represent essentially the same text, but with different actual sequences (see Unicode Normalisation Forms
%  	in  Section~\ref{Sec:normalisation} and \citealp{usa15:normalization})
%
% quote: The default for strength in UCA is tertiary; ???
%
% quote:. For example, setting the strength to exclude differences at Level 3 has the effect of ignoring case and compatibility format distinctions between letters when matching. Excluding differences at Level 2 has the effect of also ignoring accentual distinctions when matching.
%
%




\paragraph{Backward secondary sorting.}
The French Canadian Sorting Standard CAN/CSA Z243.4.1 (historically this had been
the default for all French locales) requires the word ordering with respect
to  the last accent difference. Such a behaviour can be applied
either by setting the French-Canadian locale or by passing the \code{french=TRUE}
option to the Collator.

\begin{Schunk}
\begin{Sinput}
R> stri_sort(c("cote", "côte", "coté", "côté"), locale="fr_FR")
\end{Sinput}
\begin{Soutput}
[1] "cote" "coté" "côte" "côté"
\end{Soutput}
\begin{Sinput}
R> stri_sort(c("cote", "côte", "coté", "côté"), locale="fr_CA") # french=TRUE
\end{Sinput}
\begin{Soutput}
[1] "cote" "côte" "coté" "côté"
\end{Soutput}
\end{Schunk}



\paragraph{Sorting numerals.}
Moreover, let's note the effect of setting the \code{numeric} option on the
sorting of strings that involves numbers:

\begin{Schunk}
\begin{Sinput}
R> stri_sort(c("a1", "a2", "a11", "a10", "a100"))  # lexicographic order
\end{Sinput}
\begin{Soutput}
[1] "a1"   "a10"  "a100" "a11"  "a2"
\end{Soutput}
\begin{Sinput}
R> stri_sort(c("a1", "a2", "a11", "a10", "a100"), numeric=TRUE)
\end{Sinput}
\begin{Soutput}
[1] "a1"   "a2"   "a10"  "a11"  "a100"
\end{Soutput}
\end{Schunk}

% \noindent
% Na marginesie, zaletą funkcji z~pakietu \pkg{stringi} jest nie tylko
% to, że pozwalają nam pracować zgodnie z~regułami panującymi w~praktycznie
% dowolnym miejscu na świecie (nierzadko zachodzi potrzeba sporządzenia
% analiz np.~dla oddziałów regionalnych korporacji, dla której pracujemy).
% Istotne jest także to, że często są one znacząco szybsze niż ich
% bazowe odpowiedniki.

% <<echo=-1>>=
% set.seed(123)
% slowa <- sample(readLines(
%    "https://raw.githubusercontent.com/" %s+% # złączenie napisów, zob. dalej
%    "gagolews/Programowanie_w_jezyku_R/master/wyd2/dane/PL_ispell.txt"
% )) # baza słów w języku polskim
% length(slowa)
% cat(sample(slowa, 5), sep=", ")
% print(microbenchmark::microbenchmark(
%    sort      = sort(slowa),
%    stri_sort = stri_sort(slowa),
%    times=10
% ), signif=3)
% @




\paragraph{A note on compatibility equivalence.}
In Section~\ref{Sec:normalisation} we describe different ways to normalise
canonically equivalent code point sequences so that they are
represented by the same code points, which can account for some negligible
differences (as in the ``a with ogonek'' example above).

Apart from ignoring punctuation and case, the Unicode Standard Annex \#15 \citep{usa15:normalization}
also discusses the so-called \textit{compatibility} equivalence of strings.
This is a looser form of similarity; it is observed when
there is the same \textit{abstract} content, yet displayed
by means of different glyphs, for instance ``¼'' (U+00BC) vs.~``\code{1/4}''
or ``$\mathbb{R}$'' vs.~``\proglang{R}''.
In the latter case, whether these should be treated as equal,
depends on the context (e.g., this can be the set of real numbers
vs.~one's favourite programming language).
Compatibility decompositions (NFKC, NFKD)
mentioned in Section~\ref{Sec:normalisation}
or other types of transliteration can be used to normalise strings so that
such differences are not accounted for.

Also, for ``fuzzy'' matching of strings,
the \pkg{stringdist} package \citep{stringdist} might be helpful.


% adist, agrep, \pkg{stringdist} + citation package -- "fuzzy pattern matching"
% some approximate NNs?
% \pkg{stringdist} -- out of scope of stringi




%stri_cmp_equiv("\u00bc", "1/4", strength=0, alternate_shifted=TRUE)


% > stri_trans_general("a\u0328ą", "name")
% [1] "\\N{LATIN SMALL LETTER A}\\N{COMBINING OGONEK}\\N{LATIN SMALL LETTER A WITH OGONEK}"





\subsection{String searching}

The \pkg{ICU} Collator can also be utilised
when there is a need to locate the occurrences of simple textual patterns.
All the string search functions described in Section~\ref{Sec:fixed}
have their \code{*_coll()}-suffixed equivalents.
Despite being slower than their \code{*_fixed()} counterparts,
they are more appropriate in NLP activities.

\begin{Schunk}
\begin{Sinput}
R> stri_detect_coll("Er ist so groß.", "GROSS", strength=1, locale="de_AT")
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\begin{Sinput}
R> stri_detect_coll("On je chladný", "chladny", strength=1, locale="sk_SK")
\end{Sinput}
\begin{Soutput}
[1] TRUE
\end{Soutput}
\end{Schunk}
% \end{ciekawostka}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Other operations}\label{Sec:other}

In the sequel, we cover the functions that deal with
text boundaries' detection, random string generation,
date/time formatting and parsing, amongst others.


% \color{blue}
\subsection{Analysing text boundaries}\label{Sec:BoundaryAnalysis}

Text boundary analysis aims at locating linguistic delimiters
for the purpose of word-wrapping of text, counting characters or
words, locating particular text units (e.g.,
the 3rd sentence), etc.

Generally, text boundary analysis is a locale-sensitive operation,
see \citep{usa29:segmentation}.
For example, in Japanese and Chinese, spaces are not used for
the separating of words --
a line break can occur even in the middle of a word. Nevertheless,
these languages have punctuation and diacritical marks that cannot
start or end a line, so this must also be taken into account.

The \pkg{ICU} Break Iterator\footnote{See the \pkg{ICU} User Guide
on {Boundary Analysis}, \url{http://userguide.icu-project.org/boundaryanalysis}.}
comes in four flavours (see the \code{type} option
in \code{stri_opts_brkiter()}):
\code{character}, \code{work}, \code{line_break}, and \code{sentence}.

We have access to functions such as
\code{stri_count_boundaries()},
\code{stri_split_boundaries()},
\code{stri_extract_*_boundaries()}, and
\code{stri_locate_*_boundaries()},
as well as their specialised
versions:
\code{stri_count_words()},
\code{stri_extract_*_words()}, and
\code{stri_split_lines()}, amongst others.
For example:


\begin{Schunk}
\begin{Sinput}
R> x <- "The\u00a0above-mentioned    features are useful. " %s+%
+    "My hovercraft is full of eels, eggs, and spam."
R> stri_count_boundaries(x, type="sentence")  # number of sentences
\end{Sinput}
\begin{Soutput}
[1] 2
\end{Soutput}
\begin{Sinput}
R> stri_count_boundaries(x, type="word")  # number of word boundaries
\end{Sinput}
\begin{Soutput}
[1] 36
\end{Soutput}
\begin{Sinput}
R> stri_count_words(x)                    # number of words themselves
\end{Sinput}
\begin{Soutput}
[1] 15
\end{Soutput}
\begin{Sinput}
R> stri_extract_all_words(x)
\end{Sinput}
\begin{Soutput}
[[1]]
 [1] "The"        "above"      "mentioned"  "features"   "are"
 [6] "useful"     "My"         "hovercraft" "is"         "full"
[11] "of"         "eels"       "eggs"       "and"        "spam"
\end{Soutput}
\end{Schunk}


% \code{?stri_opts_brkiter}



\subsection{Trimming, padding, and other formatting}

The following functions can be useful when pretty-printing
character strings or text on the console,
dynamically generating reports (e.g.,
with \code{Sweave()} or \pkg{knitr}; see \citealp{knitr}), or creating text files
(e.g., with \code{stri\_write\_lines()}; see Section~\ref{Sec:read_lines}).



\paragraph{Padding.}
Strings can be padded with some character so that they are of the desired
lengths by means of the \code{stri\_pad()} function.
This can be used to centre, left-, or right-align a message
when printed with, e.g.,  \code{cat()}.


\begin{Schunk}
\begin{Sinput}
R> cat(stri_pad("spam", width=77, side="left"))
\end{Sinput}
\begin{Soutput}
                                                                         spam
\end{Soutput}
\begin{Sinput}
R> cat(stri_pad("SPAMITY SPAM", width=77, side="both", pad="."))
\end{Sinput}
\begin{Soutput}
................................SPAMITY SPAM.................................
\end{Soutput}
\end{Schunk}


% <<>>=
% x <- "*" %s*% 1:3
% y <- "*" %s*% 3:1
% w <- max(stri_length(c(x, y)))
% cat(stri_paste(stri_pad(x, width=w, side="left"),
%                stri_pad(y, width=w, side="left"),
%                collapse="\n", sep=" | "))
% @





\paragraph{Trimming.}
A dual operation is that of trimming from the left or right side
of strings:

\begin{Schunk}
\begin{Sinput}
R> x <- "      spam, eggs, and lovely spam.\n"
R> stri_trim(x)  # side="both"
\end{Sinput}
\begin{Soutput}
[1] "spam, eggs, and lovely spam."
\end{Soutput}
\begin{Sinput}
R> stri_trim(x, pattern="[^\\n\\p{Z}\\p{P}\\p{S}]")
\end{Sinput}
\begin{Soutput}
[1] "spam, eggs, and lovely spam"
\end{Soutput}
\end{Schunk}



\paragraph{Word wrapping.}
The \code{stri\_wrap()} function splits each (possibly long)
string in a character vector into chunks of at most a given width or length.
By default, the dynamic word wrap algorithm \citep{Knuth:wrap}
that minimises the raggedness of the formatted text is used.
However, there is also an option (\code{cost\_exponent=0})
to use the greedy alignment,
for compatibility with the built-in \code{strwrap()}.

\begin{Schunk}
\begin{Sinput}
R> x <- stri_rand_lipsum(1)  # random text paragraph
R> cat(stri_wrap(x, width=60, indent=24, exdent=20, prefix="> "), sep="\n")
\end{Sinput}
\begin{Soutput}
>                         Lorem ipsum dolor sit amet, quis
>                     donec pretium auctor, quis id. Mauris
>                     rhoncus donec amet egestas sagittis
>                     ipsum per. Sed, sociis amet. Aliquam
>                     fusce dictumst sed vehicula ultrices
>                     arcu. Eros, netus et. Amet amet mi
>                     vestibulum vitae dapibus ut felis.
>                     Magnis in vestibulum egestas massa
>                     curabitur a ut, eget in in facilisis.
>                     Etiam odio fermentum sit ante
>                     ridiculus sit elit. Sapien torquent
>                     fermentum tortor gravida ornare sapien
>                     consequat et sem turpis. Hac vel lacus
>                     habitasse et id non. Metus habitasse
>                     sed lacinia nibh ex metus. Amet nam
>                     vestibulum ornare tincidunt massa sed
>                     ullamcorper.
\end{Soutput}
\end{Schunk}

Note that by default splitting is performed at line breaks
(compare Section~\ref{Sec:BoundaryAnalysis}).






\paragraph{Applying string templates.}
The binary operator \code{\%s\$\%} provides access to the built-in
\code{sprintf()} in a way similar to Python's \code{\%} overloaded
for objects of type \code{str}.

\begin{Schunk}
\begin{Sinput}
R> "value='%d'" %s$% 3              # equivalently: "value='%d'" %s$% list(3)
\end{Sinput}
\begin{Soutput}
[1] "value='3'"
\end{Soutput}
\begin{Sinput}
R> "%s='%d'"    %s$% list("value", 1:3)
\end{Sinput}
\begin{Soutput}
[1] "value='1'" "value='2'" "value='3'"
\end{Soutput}
\end{Schunk}

% <<>>=
% options(scipen=10)
% cat(format(c(3, 1e5, 1e-4)), sep="\n")
% @




\subsection{Generating random strings}

Apart from \code{stri\_rand\_lipsum()},
which produces random-ish text paragraphs (``placeholders'' for real text),
we have access to a function that generates sequences of characters
uniformly sampled (with replacement) from a given set.

\begin{Schunk}
\begin{Sinput}
R> stri_rand_strings(5, 8, "[actg]")
\end{Sinput}
\begin{Soutput}
[1] "ctcttagt" "gctcggat" "aacttggt" "ggggcatt" "gtactaca"
\end{Soutput}
\begin{Sinput}
R> stri_rand_strings(5, 2:6, "[A-Za-z]")
\end{Sinput}
\begin{Soutput}
[1] "HV"     "VTH"    "HMYN"   "sCWpG"  "dKGnuT"
\end{Soutput}
\begin{Sinput}
R> stri_rand_strings(1, 8, "[\\p{script=Katakana}&\\p{L}]")
\end{Sinput}
\begin{Soutput}
[1] "ｦグムノﾀルｿﾀ"
\end{Soutput}
\end{Schunk}

See Section~\ref{Sec:RegexIndividualChars} for different ways
to specify character sets.





% \color{blue}
\subsection{Transliterating}

Transliteration, in its broad sense, deals with the substitution
of characters or their groups for different ones, according to some
well-defined rules. It may be useful, amongst others, when "normalising"
pieces of strings or identifiers so that they can be more easily
compared with each other.


\paragraph{Case mapping.}
% http://www.unicode.org/versions/Unicode11.0.0/
Mapping to  upper, lower, or title case
is a language- and context-sensitive operation
that can change the total number of code points in a string.

% of a string, (character mapping may depend on its surrounding characters). http://userguide.icu-project.org/transforms/casemappings


\begin{Schunk}
\begin{Sinput}
R> stri_trans_toupper("groß")
\end{Sinput}
\begin{Soutput}
[1] "GROSS"
\end{Soutput}
\begin{Sinput}
R> stri_trans_tolower("Iİ", locale = "tr_TR")               # Turkish
\end{Sinput}
\begin{Soutput}
[1] "ıi"
\end{Soutput}
\begin{Sinput}
R> stri_trans_totitle("ijsvrij yoghurt", locale = "nl_NL")  # Dutch
\end{Sinput}
\begin{Soutput}
[1] "IJsvrij Yoghurt"
\end{Soutput}
\end{Schunk}





\paragraph{Mapping between specific characters.}
If a fast 1-to-1 exchange of characters is required, we can call:

\begin{Schunk}
\begin{Sinput}
R> stri_trans_char("GATAAATCTGGTCTTATTTCC", "ACGT", "tgca")
\end{Sinput}
\begin{Soutput}
[1] "ctatttagaccagaataaagg"
\end{Soutput}
\end{Schunk}

Here, ``\code{A}'', ``\code{C}'', ``\code{G}'', and ``\code{T}''
is replaced with
``\code{t}'', ``\code{g}'', ``\code{c}'', and ``\code{a}'', respectively.



\paragraph{General transforms.}
The \code{stri_stats_general()} function
provides access to a wide range of text transforms
defined by \pkg{ICU}\footnote{
See the \pkg{ICU} User Guide on {General Transforms},
\url{http://userguide.icu-project.org/transforms/general}.
}, whose catalogue can be accessed by calling
\code{stri_trans_list()}.

\begin{Schunk}
\begin{Sinput}
R> sample(stri_trans_list(), 9)  # a few random entries
\end{Sinput}
\begin{Soutput}
[1] "Kannada-Telugu"     "Devanagari-Arabic"  "Malayalam-Tamil"
[4] "Any-uz/BGN"         "Any-Greek"          "dv-dv_Latn/BGN"
[7] "Malayalam-Gurmukhi" "Gujr-Latn"          "Gujarati-Kannada"
\end{Soutput}
\end{Schunk}

Some examples:


\begin{Schunk}
\begin{Sinput}
R> stri_trans_general("groß© żółć La Niña köszönöm", "upper; latin-ascii")
\end{Sinput}
\begin{Soutput}
[1] "GROSS(C) ZOLC LA NINA KOSZONOM"
\end{Soutput}
\begin{Sinput}
R> stri_trans_general("Let's go... -- she said.", "any-publishing")
\end{Sinput}
\begin{Soutput}
[1] "Let’s go… — she said."
\end{Soutput}
\end{Schunk}

% stri_trans_general("stringi", "latin-greek")
% stri_trans_general("Пётр Ильич Чайковский",
%     "cyrillic-latin; nfd; [:nonspacing mark:] remove; nfc")

% not locale-dependent



% It's getting hotter with these:
%
% ```{r}
% stri_trans_general("w szczebrzeszynie chrząszcz brzmi w trzcinie", "pl-pl_fonipa")
% # and now the same in the XSampa ASCII-range representation:
% stri_trans_general("w szczebrzeszynie chrząszcz brzmi w trzcinie", "pl-pl_fonipa; ipa-xsampa")
% ```

%  stri_trans_general("zażółć gęślą jaźń", "[\\p{Z}] remove")

%  stri_trans_general("Let's go... -- that's what she said.", "any-publishing")






\subsection{Parsing and formatting date and time}\label{Sec:datetime}

In base \proglang{R}, dealing with dates and times in languages
different than the current locale is somewhat difficult.
For instance, most of the readers of this paper
may find the task of parsing the following Polish date problematic:

\begin{Schunk}
\begin{Sinput}
R> x <- "27 sierpnia 2020 r., godz. 17:17:32"
\end{Sinput}
\end{Schunk}

\pkg{stringi} connects to the \pkg{ICU} date and time
services  so that parsing/formatting temporal data from/to any locale
is possible:


\begin{Schunk}
\begin{Sinput}
R> stri_datetime_parse(x, "dd MMMM yyyy 'r., godz.' HH:mm:ss",
+    locale="pl_PL", tz="Europe/Warsaw")
\end{Sinput}
\begin{Soutput}
[1] "2020-08-27 17:17:32 CEST"
\end{Soutput}
\end{Schunk}

This function returns an object of class \code{POSIXct},
for compatibility with base \proglang{R}.
Note, however, that
\pkg{ICU} uses its own format patterns\footnote{
See the \pkg{ICU} User Guide on {Formatting Dates and Times},
\url{http://userguide.icu-project.org/formatparse/datetime}.
}. For convenience,
\code{strftime()}- and \code{strptime()}-compatible
templates can be converted with \code{stri_datetime_fstr()}:

\begin{Schunk}
\begin{Sinput}
R> stri_datetime_parse(x,
+    stri_datetime_fstr("%d %B %Y r., godz. %H:%M:%S"),
+    locale="pl_PL", tz="Europe/Warsaw")
\end{Sinput}
\begin{Soutput}
[1] "2020-08-27 17:17:32 CEST"
\end{Soutput}
\end{Schunk}


Some more examples:



\begin{Schunk}
\begin{Sinput}
R> stri_datetime_format(stri_datetime_now(), # current date and time
+    "datetime_full",                        # full format
+    locale="de_AT", tz="Europe/Vienna")
\end{Sinput}
\begin{Soutput}
[1] "Donnerstag, 17. September 2020 um 13:13:36 Mitteleuropäische Sommerzeit"
\end{Soutput}
\begin{Sinput}
R> stri_datetime_format(
+    stri_datetime_add(stri_datetime_now(), 1, "day"), # add 1 day to 'now'
+    "datetime_relative_long",              # full format, relative to 'now'
+    locale="en_NZ", tz="NZ")
\end{Sinput}
\begin{Soutput}
[1] "tomorrow at 11:13:36 PM NZST"
\end{Soutput}
\begin{Sinput}
R> stri_datetime_format(
+    stri_datetime_create(2020, 1:12, 1),   # vectorised w.r.t. all arguments
+    "date_long",                           # date only
+    locale="@calendar=hebrew")             # English locale, Hebrew calendar
\end{Sinput}
\begin{Soutput}
 [1] "4 Tevet 5780"    "6 Shevat 5780"   "5 Adar 5780"     "7 Nisan 5780"
 [5] "7 Iyar 5780"     "9 Sivan 5780"    "9 Tamuz 5780"    "11 Av 5780"
 [9] "12 Elul 5780"    "13 Tishri 5781"  "14 Heshvan 5781" "15 Kislev 5781"
\end{Soutput}
\begin{Sinput}
R> stri_datetime_format(
+    stri_datetime_create(2020, c(2, 8), c(4, 7)),
+    "date_full",
+    locale="ja_JP@calendar=japanese")      # Japanese locale and calendar
\end{Sinput}
\begin{Soutput}
[1] "平成32年2月4日火曜日" "平成32年8月7日金曜日"
\end{Soutput}
\end{Schunk}





% \code{he_IL@calendar=hebrew}
% \code{ja_JP_TRADITIONAL}

% stri_locale_info("@calendar=hebrew")

% \code{@calendar=hebrew}



% timezone



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Input and output}\label{Sec:io}

This section deals with some more advanced topics related to the
interoperability
between different platforms. In particular, we discuss
how to assure that data read from input connections are interpreted
in the correct manner.



\subsection{Dealing with Unicode code points}\label{Sec:codepoints}

The Unicode Standard (as well as the
Universal Coded Character Set, i.e., ISO/IEC 10646)
currently defines over 140{,}000 abstract characters together with
their corresponding \textit{code points} -- integers
between 0 and 1{,}114{,}111 (or 0000${}_{16}$ and 10FFFF${}_{16}$
in hexadecimal notation, see \url{https://www.unicode.org/charts/}).
In particular, here are the counts of the code points in
a few popular categories (compare Section~\ref{Sec:RegexIndividualChars}),
such as letters, numbers, and the like.

\begin{Schunk}
\begin{Sinput}
R> z <- c("\\p{L}", "\\p{Ll}", "\\p{Lu}", "\\p{N}", "\\p{P}", "\\p{S}",
+    "\\w", "\\d", "\\s")
R> structure(stri_count_regex(stri_enc_fromutf32(
+    setdiff(1:0x10ffff, c(0xd800:0xf8ff))), z), names=z)
\end{Sinput}
\begin{Soutput}
 \\p{L} \\p{Ll} \\p{Lu}  \\p{N}  \\p{P}  \\p{S}     \\w     \\d     \\s
 125093    2063    1702    1502     770    6978  128238     590      25
\end{Soutput}
\end{Schunk}

Yet, most of the code points are still unallocated -- the Unicode
standard is updated from time to time, e.g., the recent versions
were supplemented with over 1{,}000 emojis.


The first 255 code points are identical to the ones defined
by ISO/IEC 8859-1 (ISO Latin-1;
``Western European''), which itself extends US-ASCII (codes $\le 127=\text{7F}{}_{16}$).
For instance, the code point that we are used to denoting as U+007A
(the ``U+'' prefix is followed by a sequence of hexadecimal digits;
7A${}_{16}$ corresponds to decimal 122) encodes the lower case letter ``z''.
To input such a code point in \proglang{R}, we write:

\begin{Schunk}
\begin{Sinput}
R> "\u007A"  # or "\U0000007A"
\end{Sinput}
\begin{Soutput}
[1] "z"
\end{Soutput}
\end{Schunk}


For communicating with \pkg{ICU} and other libraries,
we may need to escape a given string, for example, as follows
(recall that to input a backslash in \proglang{R},
we must precede in with another backslash).

\begin{Schunk}
\begin{Sinput}
R> x <- "zß你好"
R> stri_escape_unicode(x)
\end{Sinput}
\begin{Soutput}
[1] "z\\u00df\\u4f60\\u597d"
\end{Soutput}
\begin{Sinput}
R> stri_trans_general(x, "any-hex")
\end{Sinput}
\begin{Soutput}
[1] "\\u007A\\u00DF\\u4F60\\u597D"
\end{Soutput}
\begin{Sinput}
R> stri_trans_general(x, "[^\\u0000-\\u007f] any-hex") # except ASCII
\end{Sinput}
\begin{Soutput}
[1] "z\\u00DF\\u4F60\\u597D"
\end{Soutput}
\begin{Sinput}
R> stri_trans_general(x, "[^\\u0000-\\u007f] any-hex/xml")
\end{Sinput}
\begin{Soutput}
[1] "z&#xDF;&#x4F60;&#x597D;"
\end{Soutput}
\end{Schunk}

% stri_unescape_unicode("\\u007A")

\bigskip
It is worth noting that despite the fact that some output devices
might be unable to display certain code points correctly
(due to, e.g., missing fonts), the correctness of their
processing with \pkg{stringi} is still guaranteed by \pkg{ICU}.
Here is an example of an incorrect \textit{presentation} of an emoji,
generated by a malconfigured \XeLaTeX{} engine:

\begin{Schunk}
\begin{Sinput}
R> "\U001F600" # the grinning face emoji, (:               - font unavailable
\end{Sinput}
\begin{Soutput}
[1] "😀"
\end{Soutput}
\end{Schunk}

Nevertheless, the programmatic handling of such a code point is unaffected:

\begin{Schunk}
\begin{Sinput}
R> stri_trans_general("\U001F600", "any-name") # query the character database
\end{Sinput}
\begin{Soutput}
[1] "\\N{GRINNING FACE}"
\end{Soutput}
\end{Schunk}



% ```{r}
% stri_trans_general("ą1©,", "any-name")
% stri_trans_general("\\N{LATIN SMALL LETTER SHARP S}", "name-any")
% ```
%
%




\subsection{Character encodings}\label{Sec:encoding}

When storing strings in RAM or on the disk,
we need to decide upon the actual way
of representing the code points as sequences of bytes.
The two most popular \textit{encodings} in the Unicode family are
UTF-8 and UTF-16:

\begin{Schunk}
\begin{Sinput}
R> x <- "abz0ąß你好!"
R> stri_encode(x, to="UTF-8", to_raw=TRUE)[[1]]
\end{Sinput}
\begin{Soutput}
 [1] 61 62 7a 30 c4 85 c3 9f e4 bd a0 e5 a5 bd 21
\end{Soutput}
\begin{Sinput}
R> stri_encode(x, to="UTF-16LE", to_raw=TRUE)[[1]]
\end{Sinput}
\begin{Soutput}
 [1] 61 00 62 00 7a 00 30 00 05 01 df 00 60 4f 7d 59 21 00
\end{Soutput}
\end{Schunk}

\proglang{R}'s current platform-default encoding, which we shall
refer to as the \textit{native} encoding, is defined via the
\code{LC_CTYPE} locale category in
\code{Sys.getlocale()}. This is the representation assumed,
e.g., when reading data from the standard input
or files (e.g., when \code{scan()}
is called).
% as well as written to the output devices
% (e.g., \code{print()} and \code{cat()}).
For instance, Central European versions of Windows will assume
the ``\code{windows-1250}'' code page.
OS X as well as most Linux boxes work with UTF-8 by default.

% Character vectors in \proglang{R} resemble lists of raw-type vectors
% (each ending with byte 0x00).
All strings in \proglang{R} have an associated encoding mark
which can be read by calling \code{Encoding()} or, more conveniently,
\code{stri\_enc\_mark()}.
Most importantly, strings in ASCII, ISO-8859-1 (``\code{latin1}''),
UTF-8, and the native encoding can coexist.
Whenever a non-Unicode string is passed to a \pkg{stringi} function,
it is silently converted to UTF-8 or UTF-16, depending on the requested
operation (some \pkg{ICU} services are only available for {UTF-16} data).
Over the years, this has proven a robust, efficient, and maximally portable
design choice -- Unicode can be thought of as a superset of every other encoding.
Moreover, in order to guarantee the correctness and high performance of
the string processing pipelines, \pkg{stringi} always\footnote{With a few
obvious exceptions, such as \code{stri\_encode()}.} outputs
UTF-8 data.


\subsection{Reading and writing text files and converting between encodings}\label{Sec:read_lines}

According to a report by W3Techs\footnote{See
\url{https://w3techs.com/technologies/cross/character_encoding/ranking}.},
as of 2020--09--17, 95.4\% of websites use UTF-8.
Nevertheless, encountering other encodings is still quite likely.

\paragraph{Reading and writing text files.}
If we know the encoding of a text file  in advance,
\code{stri\_read\_lines()} can be used to read
the data in a manner similar to the built-in \code{readLines()} function
(but with a much easier access to encoding conversion):

\begin{Schunk}
\begin{Sinput}
R> # see https://github.com/gagolews/stringi/tree/master/datasets
R> x <- stri_read_lines("ES_latin1.txt", encoding="ISO-8859-1")
R> head(x)  # now x is in UTF-8
\end{Sinput}
\begin{Soutput}
[1] "LOS CONSEJOS DE UN PADRE"
[2] ""
[3] ""
[4] "El León, el rey de las selvas, agonizaba en el hueco de su caverna...."
[5] ""
[6] "Á su lado estaba su hijo, el nuevo león, el rey futuro de todos los"
\end{Soutput}
\end{Schunk}


We can call \code{stri_write_lines()} to write the contents
of a character vector to a file (each string will
constitute a separate text line), with any output encoding.




\paragraph{Detecting encoding.}
However, if a file's encoding is not known in advance, there are
a certain functions that can aid in encoding detection.
First, we can read the resource in form of a raw-type vector:

% Character vectors in \proglang{R} resemble lists of raw-type vectors
% (each ended up with byte 0x00). Each string has to be properly "decoded" so that textual information may be read from such a byte stream. This "decoding scheme" is simply called a character encoding.
%
% In other words, data in computer's memory are just bytes (small integer values) – an encoding is a way to represent characters with such numbers, it is a semantic "key" to understand a given byte sequence. For example, in ISO-8859-2 (Central European), the value 177 represents Polish ``a with ogonek'', and in ISO-8859-1 (Western European), the same value meas the ``plus-minus'' sign. Thus, a character encoding is a translation scheme.

\begin{Schunk}
\begin{Sinput}
R> # see https://github.com/gagolews/stringi/tree/master/datasets
R> x <- stri_read_raw("ES_latin1.txt")
R> head(x)  # vector of type raw
\end{Sinput}
\begin{Soutput}
[1] 4c 4f 53 20 43 4f
\end{Soutput}
\end{Schunk}

Then, to guess the encoding, we can call, e.g.:

\begin{Schunk}
\begin{Sinput}
R> stri_enc_isascii(x)
\end{Sinput}
\begin{Soutput}
[1] FALSE
\end{Soutput}
\begin{Sinput}
R> stri_enc_isutf8(x)   # false positives are possible
\end{Sinput}
\begin{Soutput}
[1] FALSE
\end{Soutput}
\end{Schunk}

\noindent
Alternatively, we can use:

\begin{Schunk}
\begin{Sinput}
R> stri_enc_detect(x)  # based on heuristics
\end{Sinput}
\begin{Soutput}
[[1]]
    Encoding Language Confidence
1 ISO-8859-1       es       0.74
2 ISO-8859-2       ro       0.32
3 ISO-8859-9       tr       0.13
4   UTF-16BE                0.10
5   UTF-16LE                0.10
\end{Soutput}
\end{Schunk}

\noindent
Nevertheless, encoding detection is an operation that relies on heuristics,
therefore there is a  chance that the output might be imprecise or even
misleading.




\paragraph{Converting encodings.}
Knowing the desired source and destination encoding precisely,
\code{stri_encode()} can be called to perform the conversion.
Contrary to the build-in \code{iconv()}, which relies
on different underlying libraries, the current function is portable
across operating systems.


\begin{Schunk}
\begin{Sinput}
R> y <- stri_encode(x, from="ISO-8859-1", to="UTF-8")
R> # split into text lines
R> tail(stri_split_lines1(y))  # spoiler alert!
\end{Sinput}
\begin{Soutput}
[1] "El mono saltó sobre el perro, y en él se montó imitando al hombre;"
[2] "caballo perruno y caballero cuadrumano, salieron corriendo por el"
[3] "bosque."
[4] ""
[5] "El águila se remontó, diciendo:--El hombre mató al león; hay que subir"
[6] "mucho para que no me alcance; ¿quién sabe si algún día me alcanzará?"
\end{Soutput}
\end{Schunk}


\code{stri_enc_list()}  provides a list of
supported encodings and their aliases in many different forms.
Encoding specifiers are normalised automatically, e.g.,
\code{"utf8"} is a synonym for \code{"UTF-8"}.

% <<>>=
% stri_enc_info("utf8")
% @






% http://userguide.icu-project.org/strings/unicodeset
% http://userguide.icu-project.org/strings/properties



% \code{stri_encode()} \code{to_raw=TRUE}
%
% \code{stri_enc_get()}
%
% \code{stri_enc_toascii()}
%
% \code{stri_enc_tonative()}
%
% \code{stri_enc_toutf32()}
%
% \code{stri_enc_toutf8()}
%
% \code{stri_enc_fromutf32()}
%
%
% <<>>=
% stri_enc_info("cp1250")
% @
%
%
%
%
% \code{stri_encode()}
% \code{stri_trans_nf*()}








\subsection{Normalising strings}\label{Sec:normalisation}

In Section~\ref{Sec:Equivalence} we have provided some examples
of canonically equivalent strings whose code point representation was different.
Unicode normalisation forms C (Canonical composition, NFC) and D
(Canonical decomposition, NFD) can be applied so that they
will compare equal using bytewise matching \citep{usa15:normalization}.

\begin{Schunk}
\begin{Sinput}
R> x <- "a\u0328 ą" # a, combining ogonek, space, a with ogonek
R> stri_enc_toutf32(x)[[1]] # code points as decimals
\end{Sinput}
\begin{Soutput}
[1]  97 808  32 261
\end{Soutput}
\begin{Sinput}
R> stri_enc_toutf32(stri_trans_nfc(x))[[1]]
\end{Sinput}
\begin{Soutput}
[1] 261  32 261
\end{Soutput}
\begin{Sinput}
R> stri_enc_toutf32(stri_trans_nfd(x))[[1]]
\end{Sinput}
\begin{Soutput}
[1]  97 808  32  97 808
\end{Soutput}
\end{Schunk}

It might be a good idea to always normalise all the strings
read from external sources (files, URLs) with NFC.

Compatibility composition and decomposition normalisation forms (NFKC and
NFKD, respectively) are also available if
the removal of the formatting distinctions (font variants,
subscripts, superscripts, etc.) is expected:


\begin{Schunk}
\begin{Sinput}
R> stri_trans_nfkd("r²︷")
\end{Sinput}
\begin{Soutput}
[1] "r2{"
\end{Soutput}
\end{Schunk}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% -- Summary/conclusions/discussion -------------------------------------------

\section{Conclusion}\label{Sec:conclusions}

% Other:
%
% \code{stri_stats_general()}
% \code{stri_stats_latex()}


Over the years, many useful \proglang{R} packages related
to text processing have been developed, see \citep{textminingr,textr}.
Many of them are listed in the CRAN Task View
on Natural Language Processing, see \url{%
https://cran.r-project.org/web/views/NaturalLanguageProcessing.html}.
At the time of writing of this paper,
\pkg{stringi} itself has over 200 strong (direct) reverse dependencies.




The complete documentation of the package's API
is available at \url{https://stringi.gagolewski.com/}.
\pkg{stringi} functions can also be accessed
from within \proglang{C++} code.
See the \pkg{ExampleRcppStringi} package available at
\url{https://github.com/gagolews/ExampleRcppStringi}
for an example using \pkg{Rcpp} \citep{rcppbook}.





Finally, it is worth stressing that functions in \pkg{stringi}
are not wrappers around base \proglang{R} facilities.
A vast majority of them has been written in pure
\proglang{C} and \proglang{C++}. The operations that do not rely on \pkg{ICU}
services
have been written from scratch with speed and portability in mind.
For example, here are some timings of string concatenation:


\begin{Schunk}
\begin{Sinput}
R> x <- stri_rand_strings(length(LETTERS)*1000, 1000)
R> microbenchmark::microbenchmark(
+    join2=stri_join(LETTERS, x, sep="", collapse=", "),
+    join3=stri_join(x, LETTERS, x, sep="", collapse=", "),
+    r_paste2=paste(LETTERS, x, sep="", collapse=", "),
+    r_paste3=paste(x, LETTERS, x, sep="", collapse=", ")
+  )
\end{Sinput}
\begin{Soutput}
Unit: milliseconds
     expr     min      lq    mean  median      uq     max neval
    join2  37.713  39.040  52.593  40.208  78.734  98.048   100
    join3  74.633  87.306  91.358  88.913  97.256 135.891   100
 r_paste2  98.916 104.435 120.786 108.456 151.921 169.785   100
 r_paste3 211.733 219.606 256.273 273.239 281.927 302.317   100
\end{Soutput}
\end{Schunk}

Another example -- timings of fixed pattern searching:

\begin{Schunk}
\begin{Sinput}
R> x <- stri_rand_strings(100, 100000, "[actg]")
R> y <- "acca"
R> microbenchmark::microbenchmark(
+    fixed=stri_locate_all_fixed(x, y),
+    regex=stri_locate_all_regex(x, y),
+    coll=stri_locate_all_coll(x, y),
+    r_tre=gregexpr(y, x),
+    r_pcre=gregexpr(y, x, perl=TRUE),
+    r_fixed=gregexpr(y, x, fixed=TRUE)
+  )
\end{Sinput}
\begin{Soutput}
Unit: milliseconds
    expr      min       lq     mean   median       uq     max neval
   fixed   4.7318   4.9162   5.0106   5.0096   5.1076   5.515   100
   regex 117.0966 119.6360 121.8881 121.3912 123.5901 143.964   100
    coll 378.5757 385.3210 392.0872 392.3554 396.7838 425.230   100
   r_tre 122.3811 125.4567 127.8202 127.6302 129.4380 138.540   100
  r_pcre  75.6733  78.0173  79.1578  79.1838  80.0254  86.130   100
 r_fixed  51.4335  52.7493  53.5762  53.6306  54.3305  57.827   100
\end{Soutput}
\end{Schunk}



% The functions are, of course, \pkg{magrittr}'s pipe operator-friendly:
%
% % <<>>=
% % library("magrittr")
% % y %>% stri_sub_replace(24, length=5, replacement="spam") # bacon → spam
% % @
%
%
% <<>>=
% library("magrittr")
% y <- "spam, spam, eggs, bacon, and tasty spam"
% y %>% stri_locate_all(., fixed="spam") %>% stri_sub_replace_all(y, ., replacement="ham")
% @





% TODO: check CITATION
%
% https://cran.r-project.org/web/packages/qdapRegex/index.html
% https://cran.r-project.org/web/packages/namedCapture/index.html
% https://cran.r-project.org/web/packages/rebus/index.html
% https://cran.r-project.org/web/packages/rematch2/index.html
% https://cran.r-project.org/web/packages/rex/index.html
% https://cran.r-project.org/web/packages/RVerbalExpressions/index.html
% https://cran.r-project.org/web/packages/inverseRegex/index.html
%
% other R packages TODO check CITATION
% https://cran.r-project.org/web/packages/rprintf/index.html
% https://cran.r-project.org/web/packages/ScrabbleScore/index.html
% https://cran.r-project.org/web/packages/snakecase/index.html
% https://cran.r-project.org/web/packages/strex/index.html
% https://cran.r-project.org/web/packages/stringb/index.html
% https://cran.r-project.org/web/packages/stringdist/index.html
% https://cran.r-project.org/web/packages/stringfish/index.html
% https://cran.r-project.org/web/packages/stringformattr/index.html
% https://cran.r-project.org/web/packages/textutils/index.html
% https://cran.r-project.org/web/packages/tidyselect/index.html
% https://cran.r-project.org/web/packages/tidystringdist/index.html
% https://cran.r-project.org/web/packages/unglue/index.html
% https://cran.r-project.org/web/packages/uniqtag/index.html
% https://cran.r-project.org/web/packages/yasp/index.html
% https://cran.r-project.org/web/packages/clustringr/index.html # genie supports clustering as well
% https://cran.r-project.org/web/packages/concatenate/index.html
% https://cran.r-project.org/web/packages/cwhmisc/index.html
% https://cran.r-project.org/web/packages/fansi/index.html
% https://cran.r-project.org/web/packages/filesstrings/index.html
% https://cran.r-project.org/web/packages/fuzzywuzzyR/index.html
% https://cran.r-project.org/web/packages/glue/index.html
% https://cran.r-project.org/web/packages/GrpString/index.html
% https://cran.r-project.org/web/packages/gsubfn/index.html
% https://cran.r-project.org/web/packages/mgsub/index.html
% https://cran.r-project.org/web/packages/rprintf/index.html
% and of course
% https://cran.r-project.org/web/packages/stringr/index.html





% When a character vector argument is expected, factors and other vectors coercible to characters vectors are silently converted with as.character, otherwise an error is generated. Coercion from a list of non-atomic vectors each of length 1 issues a warning.
%
% When a logical, numeric, or integer vector argument is expected, factors are converted with as.*(as.character(...)), and other coercible vectors are converted with as.*, otherwise an error is generated.
%
% Generally, all our functions drop input objects' attributes (e.g., names, dim, etc.). This is generally because of advanced vectorization and for efficiency reasons. Thus, if arguments' preserving is needed, please remember to copy important attributes manually or use, e.g., the subsetting operation like x[] <- stri_...(x, ...).



\bigskip
Future work will involve the porting of \pkg{stringi} to different
scientific/statistical computing environments, including \proglang{Python}
with the \pkg{NumPy} \citep{numpy} ecosystem,
so as to provide more Unicode-aware
alternatives to the vectorised text processing
facilities from \code{numpy.char} and \pkg{pandas} \citep[Chap.~7]{pandas}.
% PyICU
Moreover, further extension of \pkg{stringi}'s API
so as to  provide an even broader coverage of \pkg{ICU} services
shall be conveyed.
% including number and message formatting



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section*{Acknowledgements}

First and foremost, the author wishes to thank Hadley Wickham for coming
up with the \pkg{stringr} package API and bringing the idea of ``tidying up''
\proglang{R} string processing workflows.
Also, many thanks to all the contributors who have donated their
time and effort (in all the possible forms: code, feature suggestions,
ideas, criticism) to make \pkg{stringi} better --
Bartek Tartanus,
Kenneth Benoit,
Marcin Bujarski,
Bill Denney,
Katrin Leinweber,
Jeroen Ooms,
Davis Vaughan,
and many others,
see \url{https://github.com/gagolews/stringi/graphs/contributors}.
More contributions are always welcome.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{refs}

\end{document}
